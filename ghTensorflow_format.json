{
    "Data Name": "Github Data for Tensorflow",
    "Creation Date": "2015-11-07T01:19:20Z",
    "Data Range Start": "2015-11-09T14:21:11Z",
    "Data Range End": "2022-11-21T23:41:29Z",
    "Version Information": 1.0,
    "Provenance Information": "Utlizes data/gh_Issues.json, data/gh_Pulls.json and dynamic requests to the Github API",
    "Predicted Symbols": [
        "tbd"
    ],
    "Prediction Period": "tbd",
    "Entities": [
        {
            "Id": 55463253,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T23:41:29Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T23:41:29Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2019-09-17T20:24:33Z",
                    "Valid To": "2022-11-21T13:58:59Z"
                }
            ]
        },
        {
            "Id": 19536781,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T23:41:06Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2016-05-23T17:05:35Z",
                    "Valid To": "2022-06-17T16:40:30Z"
                }
            ]
        },
        {
            "Id": 38714576,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T23:17:42Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2018-04-25T05:53:01Z",
                    "Valid To": "2022-11-21T23:24:12Z"
                }
            ]
        },
        {
            "Id": 29215195,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T22:35:25Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T22:35:25Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2017-06-05T23:20:45Z",
                    "Valid To": "2022-10-24T17:00:08Z"
                }
            ]
        },
        {
            "Id": 118690585,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T22:09:56Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T22:09:56Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2022-11-20T20:06:49Z",
                    "Valid To": "2022-11-20T20:08:24Z"
                }
            ]
        },
        {
            "Id": 26332583,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T20:58:44Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T20:58:44Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2017-03-10T18:34:15Z",
                    "Valid To": "2022-11-21T21:01:57Z"
                }
            ]
        },
        {
            "Id": 6442822,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T19:40:05Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T19:40:05Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2014-01-19T12:31:04Z",
                    "Valid To": "2022-11-17T20:55:59Z"
                }
            ]
        },
        {
            "Id": 67419721,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T18:37:33Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-06-25T11:49:21Z",
                    "Valid To": "2022-08-09T12:24:43Z"
                }
            ]
        },
        {
            "Id": 105635332,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T16:47:45Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2022-05-16T08:55:57Z",
                    "Valid To": "2022-11-22T07:45:41Z"
                }
            ]
        },
        {
            "Id": 74714236,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T14:03:03Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2020-11-19T11:33:34Z",
                    "Valid To": "2022-11-09T09:22:45Z"
                }
            ]
        },
        {
            "Id": 96806369,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T12:00:10Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2021-12-29T03:19:39Z",
                    "Valid To": "2022-10-07T10:32:14Z"
                }
            ]
        },
        {
            "Id": 913790,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T11:20:11Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T11:20:11Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2011-07-13T22:26:34Z",
                    "Valid To": "2022-11-05T14:37:56Z"
                }
            ]
        },
        {
            "Id": 92745698,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T10:13:28Z",
                    "Valid To": "2022-11-21T10:33:29Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2021-10-18T16:30:39Z",
                    "Valid To": "2022-11-20T12:28:07Z"
                }
            ]
        },
        {
            "Id": 24700291,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T09:40:36Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Average Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2016-12-21T15:16:00Z",
                    "Valid To": "2022-11-18T08:28:25Z"
                }
            ]
        },
        {
            "Id": 44562106,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T09:31:58Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2018-10-29T04:17:10Z",
                    "Valid To": "2022-10-29T14:07:54Z"
                }
            ]
        },
        {
            "Id": 57779173,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T06:35:21Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2019-11-15T01:01:23Z",
                    "Valid To": "2022-11-22T00:47:37Z"
                }
            ]
        },
        {
            "Id": 100826172,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T05:30:01Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2022-03-03T06:06:55Z",
                    "Valid To": "2022-11-21T05:29:54Z"
                }
            ]
        },
        {
            "Id": 15100009,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T00:36:18Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T00:36:18Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "More Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2015-10-13T04:29:58Z",
                    "Valid To": "2022-11-03T10:51:44Z"
                }
            ]
        },
        {
            "Id": 15100009,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-21T00:19:21Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-21T00:19:21Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "More Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2015-10-13T04:29:58Z",
                    "Valid To": "2022-11-03T10:51:44Z"
                }
            ]
        },
        {
            "Id": 2053858,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-20T18:44:28Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Average Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2012-07-27T20:00:15Z",
                    "Valid To": "2022-11-20T18:20:32Z"
                }
            ]
        },
        {
            "Id": 38817737,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-20T16:15:50Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2018-04-28T18:49:19Z",
                    "Valid To": "2022-10-15T07:50:02Z"
                }
            ]
        },
        {
            "Id": 26105224,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-20T15:45:02Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2017-03-01T05:21:02Z",
                    "Valid To": "2022-11-21T11:56:43Z"
                }
            ]
        },
        {
            "Id": 17670772,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-20T13:16:10Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2016-03-05T10:16:39Z",
                    "Valid To": "2022-10-20T13:31:02Z"
                }
            ]
        },
        {
            "Id": 22030060,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-20T03:29:04Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2016-09-06T14:07:30Z",
                    "Valid To": "2022-09-21T02:06:47Z"
                }
            ]
        },
        {
            "Id": 53497039,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-19T16:34:26Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Average Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2019-07-30T22:20:52Z",
                    "Valid To": "2022-11-11T15:01:36Z"
                }
            ]
        },
        {
            "Id": 2458806,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-19T16:28:39Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Average Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2012-09-30T19:43:51Z",
                    "Valid To": "2022-11-19T16:19:31Z"
                }
            ]
        },
        {
            "Id": 33950866,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-19T12:53:43Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-19T12:53:43Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2017-11-24T07:15:21Z",
                    "Valid To": "2022-04-06T01:58:36Z"
                }
            ]
        },
        {
            "Id": 33950866,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-19T12:36:22Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2017-11-24T07:15:21Z",
                    "Valid To": "2022-04-06T01:58:36Z"
                }
            ]
        },
        {
            "Id": 42224728,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-19T06:09:14Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-19T06:09:14Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2018-08-09T00:36:38Z",
                    "Valid To": "2022-06-29T17:55:37Z"
                }
            ]
        },
        {
            "Id": 59843233,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T16:08:12Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-01-13T18:29:24Z",
                    "Valid To": "2022-11-18T14:19:38Z"
                }
            ]
        },
        {
            "Id": 14215174,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T15:42:56Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2015-09-10T09:26:48Z",
                    "Valid To": "2022-03-15T13:36:14Z"
                }
            ]
        },
        {
            "Id": 15980055,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T15:07:02Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2015-11-23T11:45:40Z",
                    "Valid To": "2022-11-18T17:08:28Z"
                }
            ]
        },
        {
            "Id": 67419721,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T13:38:41Z",
                    "Valid To": "2022-11-21T18:40:40Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-06-25T11:49:21Z",
                    "Valid To": "2022-08-09T12:24:43Z"
                }
            ]
        },
        {
            "Id": 95025816,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T12:10:40Z",
                    "Valid To": "2022-11-20T08:32:53Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2021-11-25T10:30:17Z",
                    "Valid To": "2022-11-20T08:46:13Z"
                }
            ]
        },
        {
            "Id": 87115287,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T08:48:08Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2021-07-08T06:59:57Z",
                    "Valid To": "2022-11-22T04:11:06Z"
                }
            ]
        },
        {
            "Id": 12981474,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T00:23:19Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-18T00:23:19Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-18T09:05:26Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "More Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2015-06-20T19:52:02Z",
                    "Valid To": "2022-11-01T18:38:04Z"
                }
            ]
        },
        {
            "Id": 56741989,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-18T00:23:19Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-18T00:23:19Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-18T09:05:26Z",
                    "Valid To": "2022-11-18T09:05:26Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2019-10-18T20:47:54Z",
                    "Valid To": "2019-10-18T20:47:54Z"
                }
            ]
        },
        {
            "Id": 38138022,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-17T16:31:00Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": null,
                    "Valid From": "2018-04-06T15:51:35Z",
                    "Valid To": "2022-07-28T16:56:20Z"
                }
            ]
        },
        {
            "Id": 61427290,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-17T10:17:39Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-02-24T17:14:57Z",
                    "Valid To": "2022-10-28T13:47:50Z"
                }
            ]
        },
        {
            "Id": 86464649,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-17T08:54:58Z",
                    "Valid To": "2022-11-17T12:21:58Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-17T08:54:58Z",
                    "Valid To": "2022-11-17T12:21:58Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2021-06-25T07:31:47Z",
                    "Valid To": "2022-11-07T02:53:28Z"
                }
            ]
        },
        {
            "Id": 20158647,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T18:19:09Z",
                    "Valid To": "2022-11-16T19:27:58Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2016-06-27T02:02:32Z",
                    "Valid To": "2022-09-29T14:50:26Z"
                }
            ]
        },
        {
            "Id": 25738889,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T18:11:12Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2017-02-13T09:01:51Z",
                    "Valid To": "2022-02-17T14:37:15Z"
                }
            ]
        },
        {
            "Id": 8885699,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T15:23:07Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2014-09-23T19:35:30Z",
                    "Valid To": "2022-11-15T18:44:46Z"
                }
            ]
        },
        {
            "Id": 78156688,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T14:16:37Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T14:16:37Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2021-01-28T14:07:17Z",
                    "Valid To": "2022-11-22T18:54:56Z"
                }
            ]
        },
        {
            "Id": 60800749,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T13:54:48Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T13:54:48Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-16T21:23:13Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-02-08T00:01:52Z",
                    "Valid To": "2022-08-17T13:17:41Z"
                }
            ]
        },
        {
            "Id": 56741989,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T13:54:48Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T13:54:48Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-16T21:23:13Z",
                    "Valid To": "2022-11-16T21:23:13Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2019-10-18T20:47:54Z",
                    "Valid To": "2019-10-18T20:47:54Z"
                }
            ]
        },
        {
            "Id": 9656425,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T09:33:42Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2014-11-10T12:48:12Z",
                    "Valid To": "2022-10-18T06:09:05Z"
                }
            ]
        },
        {
            "Id": 106367904,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T01:16:16Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T01:16:16Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-16T01:17:12Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2022-05-27T05:25:55Z",
                    "Valid To": "2022-11-11T02:52:41Z"
                }
            ]
        },
        {
            "Id": 323199,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T01:16:16Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T01:16:16Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                },
                {
                    "Contribution Type": "Pull Merger",
                    "Valid From": "2022-11-16T01:17:12Z",
                    "Valid To": "2022-11-16T01:17:12Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "More Followers",
                    "Committer Type": "Heavy Committer",
                    "Valid From": "2010-07-05T11:26:31Z",
                    "Valid To": "2022-10-29T18:11:58Z"
                }
            ]
        },
        {
            "Id": 3444368,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-16T01:02:38Z",
                    "Valid To": null
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-16T01:02:38Z",
                    "Valid To": null
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2013-02-01T02:32:23Z",
                    "Valid To": "2022-11-16T00:55:46Z"
                }
            ]
        },
        {
            "Id": 99087793,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-15T23:32:04Z",
                    "Valid To": "2022-11-16T04:31:19Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-15T23:32:04Z",
                    "Valid To": "2022-11-16T04:31:19Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2022-02-05T16:05:26Z",
                    "Valid To": "2022-10-24T19:37:54Z"
                }
            ]
        },
        {
            "Id": 99087793,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-15T23:27:47Z",
                    "Valid To": "2022-11-16T04:31:23Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-15T23:27:47Z",
                    "Valid To": "2022-11-16T04:31:23Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2022-02-05T16:05:26Z",
                    "Valid To": "2022-10-24T19:37:54Z"
                }
            ]
        },
        {
            "Id": 73069040,
            "Symbols": [
                {
                    "Contribution Type": "Issue Creator",
                    "Valid From": "2022-11-15T21:19:41Z",
                    "Valid To": "2022-11-15T21:41:56Z"
                },
                {
                    "Contribution Type": "Pull Requestor",
                    "Valid From": "2022-11-15T21:19:41Z",
                    "Valid To": "2022-11-15T21:41:56Z"
                }
            ],
            "Properties": [
                {
                    "Follower type": "Less Followers",
                    "Committer Type": "Light Committer",
                    "Valid From": "2020-10-18T14:47:17Z",
                    "Valid To": "2022-11-10T23:29:26Z"
                }
            ]
        }
    ],
    "Data": [
        {
            "Timestamp": "2022-11-21T23:41:29Z",
            "EntityIds": [
                55463253,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "XYSEL4BZJM75"
            ],
            "Context": "Support the quantization of custom ops. Propagates the _tfl_quant_trait as attribute into the newly created custom op instead of as custom_option and add support for string custom options. It also forces the pruning of unused custom ops in the PostQuantizePass by assuming all custom ops have no side effect. This is a temporary solution due to the difficulty to set the enable-no-side-effect option through the Python API.\r\n"
        },
        {
            "Timestamp": "2022-11-21T23:41:29Z",
            "Entity Ids": [
                55463253,
                48215717
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "XYSEL4BZJM75",
                "OS6GB6DHXNXG"
            ],
            "Context": "Support the quantization of custom ops. Propagates the _tfl_quant_trait as attribute into the newly created custom op instead of as custom_option and add support for string custom options. It also forces the pruning of unused custom ops in the PostQuantizePass by assuming all custom ops have no side effect. This is a temporary solution due to the difficulty to set the enable-no-side-effect option through the Python API.\r\n"
        },
        {
            "Timestamp": "2022-11-21T23:41:06Z",
            "EntityIds": [
                19536781,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "OM2CAPU6RPOM"
            ],
            "Context": "MoviNet has memory leak. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.10.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10 (recreated in Ubuntu 20.4)\r\n\r\n### Mobile device\r\n\r\nNA\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n11.2 / 8.1\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nRepeated inference calls to MoviNet (loaded from SavedModel) continuously increase memory usage until a crash occurs.\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n\r\nThe process to recreate is somewhat long, so full code cannot be provided. The steps are:\r\n\r\n1. Create movinet network:\r\n```shell\r\n    backbone = movinet.Movinet(model_id=model_id)   # output_states=False\r\n    model = movinet_model.MovinetClassifier(backbone=backbone, num_classes=600, output_states=False)\r\n    model.build([1, 1, 1, 1, 3])\r\n\r\n    checkpoint_dir = download_and_extract_file(model_id=model_id)\r\n    checkpoint_path = os.path.join(checkpoint_dir, 'ckpt-1')\r\n    checkpoint = tf.train.Checkpoint(model=model)\r\n    status = checkpoint.restore(checkpoint_path)\r\n    status.assert_existing_objects_matched()\r\n\r\n    model = build_classifier(backbone=backbone, input_shape= input_shape, num_classes=n_output_classes,\r\n                             movinet_layers_to_freeze = movinet_layers_to_freeze)\r\n```\r\n\r\n2. Fine-tune the model\r\n\r\n3. Convert to savedmodel:\r\n```\r\ntf.keras.models.save_model(self.net, path, signatures=None)\r\n```\r\n\r\n4. Load savedmodel:\r\n```\r\nmodel = tf.keras.models.load_model(savedmodel_path, custom_objects={'tf': tf, 'K': tf.keras.backend})\r\n```\r\n\r\n5. Perform inference in a for loop:\r\n```\r\nfor i in range(n_samples):\r\n    out[i] = model(input[i], training=training)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\nI have ran this from my Windows PC, as well as EC2 instances with and without a GPU. I used ClearML to log machine stats over iterations and saw the following:\r\n\r\nCPU-Only:\r\n![image](https://user-images.githubusercontent.com/19536781/203179971-490e64f6-b198-43c6-a2dc-500c8190ccae.png)\r\n\r\nNote that in the beginning, there is alot of writing to disk (orange) while RAM stays constant (blue). After a certain amount of writing to disk, RAM starts being used until it is depleted and program crashes. \r\n\r\nGPU:\r\nThe same behavior appears on GPU machines, with the addition that when it starts writing to RAM, GPU starts filling up as well (top plot, blue/green lines):\r\n![image](https://user-images.githubusercontent.com/19536781/203180382-19affc22-bd43-4dc0-8a48-e97e7e80422d.png)\r\n\r\nI have tried this on my windows machine as well, and observed the same behavior (RAM usage increasing until crash occurs) in Task Manager. \r\n\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-21T23:17:42Z",
            "EntityIds": [
                38714576,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "3Y4LW0ZI9XUZ"
            ],
            "Context": "tf.io.gfile.rename cause UnicodeError on non-English Windows. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\ntensorflow-2.9.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 10 Family Chinese\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10.6 (conda)\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nThe problem is first caused by the rename issue at https://github.com/tensorflow/tensorflow/issues/45980\r\n\r\nEven when this rename error happen, it should produce \r\n\r\n```\r\ntensorflow.python.framework.errors_impl.UnknownError: Failed to rename: <src> <tar> : Access is denied.\r\n; Input/output error`\r\n```\r\n\r\nHowever, when the language of Windows is set to be non-English language, like Chinese, it cause `UnicodeError` like\r\n\r\n```\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbe in position 143: invalid start byte\r\n```\r\n\r\nI guess the reason is that, the `\"Access is denied\"` is actually the error message formatted by Win32 `FormatError()` (https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-formatmessage). When `dwLanguageId` is set to be something like `MAKELANGID(NEUTRAL, SUBLANG_DEFAULT)`, it will produce a Chinese encoding error message `\"\u62d2\u7edd\u8bbf\u95ee\"` in `cp936` (`gbk`) encoding when the system code page is `936` (Simplified Chinese with `gbk` encoding). When you want to decode it as python string using `utf-8` encoding, it cause this problem. \r\n\r\nNote `\"\u62d2\u7edd\u8bbf\u95ee\"` in `gbk` is `\"\\xbe\\xdc\\xbe\\xf8\\xb7\\xc3\\xce\\xca\"`.\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nJust follow the official tutorial at https://www.tensorflow.org/tutorials/images/data_augmentation\r\n\r\n```\r\n(train_ds, val_ds, test_ds), metadata = tfds.load(\r\n    'tf_flowers',\r\n    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\r\n    with_info=True,\r\n    as_supervised=True,\r\n)\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```\r\nWARNING:tensorflow:From <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse eager execution and: \r\n`tf.data.TFRecordDataset(path)`\r\n\r\n---------------------------------------------------------------------------\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\nCell In [2], line 3\r\n      1 # Load Data\r\n----> 3 (train_ds, val_ds, test_ds), metadata = tfds.load(\r\n      4     'tf_flowers',\r\n      5     split = ['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\r\n      6     with_info = True,\r\n      7     as_supervised = True,\r\n      8 )\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52, in disallow_positional_args.<locals>.disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50 _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51 _check_required(fn, kwargs)\r\n---> 52 return fn(*args, **kwargs)\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py:300, in load(name, split, data_dir, batch_size, in_memory, shuffle_files, download, as_supervised, decoders, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\r\n    298 if download:\r\n    299   download_and_prepare_kwargs = download_and_prepare_kwargs or {}\r\n--> 300   dbuilder.download_and_prepare(**download_and_prepare_kwargs)\r\n    302 if as_dataset_kwargs is None:\r\n    303   as_dataset_kwargs = {}\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\api_utils.py:52, in disallow_positional_args.<locals>.disallow_positional_args_dec(fn, instance, args, kwargs)\r\n     50 _check_no_positional(fn, args, ismethod, allowed=allowed)\r\n     51 _check_required(fn, kwargs)\r\n---> 52 return fn(*args, **kwargs)\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:281, in DatasetBuilder.download_and_prepare(self, download_dir, download_config)\r\n    278 self._log_download_bytes()\r\n    280 # Create a tmp dir and rename to self._data_dir on successful exit.\r\n--> 281 with file_format_adapter.incomplete_dir(self._data_dir) as tmp_data_dir:\r\n    282   # Temporarily assign _data_dir to tmp_data_dir to avoid having to forward\r\n    283   # it to every sub function.\r\n    284   with utils.temporary_assignment(self, \"_data_dir\", tmp_data_dir):\r\n    285     self._download_and_prepare(\r\n    286         dl_manager=dl_manager,\r\n    287         download_config=download_config)\r\n\r\nFile <conda_env_path>\\lib\\contextlib.py:142, in _GeneratorContextManager.__exit__(self, typ, value, traceback)\r\n    140 if typ is None:\r\n    141     try:\r\n--> 142         next(self.gen)\r\n    143     except StopIteration:\r\n    144         return False\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:200, in incomplete_dir(dirname)\r\n    198 try:\r\n    199   yield tmp_dir\r\n--> 200   tf.io.gfile.rename(tmp_dir, dirname)\r\n    201 finally:\r\n    202   if tf.io.gfile.exists(tmp_dir):\r\n\r\nFile <conda_env_path>\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:620, in rename_v2(src, dst, overwrite)\r\n    607 @tf_export(\"io.gfile.rename\")\r\n    608 def rename_v2(src, dst, overwrite=False):\r\n    609   \"\"\"Rename or move a file / directory.\r\n    610 \r\n    611   Args:\r\n   (...)\r\n    618     errors.OpError: If the operation fails.\r\n    619   \"\"\"\r\n--> 620   _pywrap_file_io.RenameFile(\r\n    621       compat.path_to_bytes(src), compat.path_to_bytes(dst), overwrite)\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xbe in position 143: invalid start byte\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-21T22:35:25Z",
            "EntityIds": [
                29215195,
                38085909,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "4UFYSKJ7DOGI"
            ],
            "Context": "[INTEL oneDNN] OneDNN Refactoring Part 1: Remove obsolete operations. This PR removes obsolete oneDNN operations (oneDNN blocked format related) as well as MKL layout pass tests and conversion tests (test cases are all related to blocked format).\r\n\r\nResults:\r\n   1. 12 source files are deleted\r\n   2. 8 source files are update (bazel files, tests)\r\n   3. Total 8843 lines of code are cleaned away!"
        },
        {
            "Timestamp": "2022-11-21T22:35:25Z",
            "Entity Ids": [
                29215195,
                38085909,
                48215717,
                38085909
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "4UFYSKJ7DOGI",
                "YRP7YHL7RL1C"
            ],
            "Context": "[INTEL oneDNN] OneDNN Refactoring Part 1: Remove obsolete operations. This PR removes obsolete oneDNN operations (oneDNN blocked format related) as well as MKL layout pass tests and conversion tests (test cases are all related to blocked format).\r\n\r\nResults:\r\n   1. 12 source files are deleted\r\n   2. 8 source files are update (bazel files, tests)\r\n   3. Total 8843 lines of code are cleaned away!"
        },
        {
            "Timestamp": "2022-11-21T22:09:56Z",
            "EntityIds": [
                118690585,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "2P87O3MI0LYV"
            ],
            "Context": "[INTEL oneDNN] OneDNN Refactoring Part 1: Remove obsolete operations. This PR removes obsolete oneDNN operations (oneDNN blocked format related) as well as MKL layout pass tests and conversion tests (test cases are all related to blocked format).\r\n\r\nResults:\r\n   1. 12 source files are deleted\r\n   2. 8 source files are update (bazel files, tests)\r\n   3. Total 8843 lines of code are cleaned away!"
        },
        {
            "Timestamp": "2022-11-21T22:09:56Z",
            "Entity Ids": [
                118690585,
                48215717
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "2P87O3MI0LYV",
                "0N094RI75909"
            ],
            "Context": "[INTEL oneDNN] OneDNN Refactoring Part 1: Remove obsolete operations. This PR removes obsolete oneDNN operations (oneDNN blocked format related) as well as MKL layout pass tests and conversion tests (test cases are all related to blocked format).\r\n\r\nResults:\r\n   1. 12 source files are deleted\r\n   2. 8 source files are update (bazel files, tests)\r\n   3. Total 8843 lines of code are cleaned away!"
        },
        {
            "Timestamp": "2022-11-21T20:58:44Z",
            "EntityIds": [
                26332583,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KV3KP435O29H"
            ],
            "Context": "Change per-process mem to use total memory as base. This is to address [this issue](https://github.com/google/jax/issues/4310).\r\nInstead of using free_memory to calculate the amount of memory to assign to the process, we need to use total_memory to be consistent with tf core.\r\nDoc is also updated to reflect it.\r\nThis needs to be included in the release doc too."
        },
        {
            "Timestamp": "2022-11-21T20:58:44Z",
            "Entity Ids": [
                26332583,
                48215717,
                1835471
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "KV3KP435O29H",
                "GQSMKGLH2ZSZ"
            ],
            "Context": "Change per-process mem to use total memory as base. This is to address [this issue](https://github.com/google/jax/issues/4310).\r\nInstead of using free_memory to calculate the amount of memory to assign to the process, we need to use total_memory to be consistent with tf core.\r\nDoc is also updated to reflect it.\r\nThis needs to be included in the release doc too."
        },
        {
            "Timestamp": "2022-11-21T19:40:05Z",
            "EntityIds": [
                6442822,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "C8PAWINGYWIT"
            ],
            "Context": "Change per-process mem to use total memory as base. This is to address [this issue](https://github.com/google/jax/issues/4310).\r\nInstead of using free_memory to calculate the amount of memory to assign to the process, we need to use total_memory to be consistent with tf core.\r\nDoc is also updated to reflect it.\r\nThis needs to be included in the release doc too."
        },
        {
            "Timestamp": "2022-11-21T19:40:05Z",
            "Entity Ids": [
                6442822,
                48215717,
                1174378,
                38085909
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "C8PAWINGYWIT",
                "E0T9LU9FWGG1"
            ],
            "Context": "Change per-process mem to use total memory as base. This is to address [this issue](https://github.com/google/jax/issues/4310).\r\nInstead of using free_memory to calculate the amount of memory to assign to the process, we need to use total_memory to be consistent with tf core.\r\nDoc is also updated to reflect it.\r\nThis needs to be included in the release doc too."
        },
        {
            "Timestamp": "2022-11-21T18:37:33Z",
            "EntityIds": [
                67419721,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "ZVTG66ZMJTJG"
            ],
            "Context": "CMake build system Tensorflow Lite v2.11.0 is broken. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBuild/Install\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.11.0\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nAndroid\r\n\r\n### Mobile device\r\n\r\nN/A\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\nN/A\r\n\r\n### GCC/Compiler version\r\n\r\nAndroid NDK 24\r\n\r\n### CUDA/cuDNN version\r\n\r\nN/A\r\n\r\n### GPU model and memory\r\n\r\nN/A\r\n\r\n### Current Behaviour?\r\n\r\nUnable to cross compile TFLite 2.11.0 for Android, using CMake.\r\n\r\nHere is the command I used to build:\r\n\r\n```shell\r\nmkdir tflite.build.android && cd tflite.build.android\r\n\r\ncmake -G \"Unix Makefiles\" -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DANDROID_STL=c++_shared -DANDROID_NATIVE_API_LEVEL=27 -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_TOOLCHAIN_FILE=$HOME/Android/Sdk/ndk/24.0.8215888/build/cmake/android.toolchain.cmake -DTFLITE_ENABLE_XNNPACK=ON -DTFLITE_ENABLE_GPU=ON -DTFLITE_ENABLE_NNAPI=ON -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF ../tensorflow/tensorflow/lite\r\n\r\nmake -j4\r\n```\r\n, and I get the following error:\r\n```\r\n 28%] Linking CXX executable flatc\r\ncd /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build && /usr/local/bin/cmake -E cmake_link_script CMakeFiles/flatc.dir/link.txt --verbose=1\r\n/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android27 --sysroot=/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/sysroot -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security -fexceptions -frtti -stdlib=libc++ -O3 -DNDEBUG -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Qunused-arguments -Wl,--no-undefined  -Wl,--gc-sections CMakeFiles/flatc.dir/src/idl_parser.cpp.o CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o CMakeFiles/flatc.dir/src/reflection.cpp.o CMakeFiles/flatc.dir/src/util.cpp.o CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o CMakeFiles/flatc.dir/src/flatc.cpp.o CMakeFiles/flatc.dir/src/flatc_main.cpp.o CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o CMakeFiles/flatc.dir/src/code_generators.cpp.o CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o -o flatc   -latomic -lm \r\nRunning scripts/generate_code.py...\r\ncd /home/myname/myworkingdir/tflite.build.android/flatbuffers && /usr/bin/python3.9 scripts/generate_code.py --flatc /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc --skip-gen-reflection\r\nTraceback (most recent call last):\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 148, in <module>\r\n    flatc(\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 82, in flatc\r\n    result = subprocess.run(cmd, cwd=str(cwd), check=True)\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 505, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nOSError: [Errno 8] Exec format error: '/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc'\r\nmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:566: _deps/flatbuffers-build/flatc] Error 1\r\nmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'\r\nmake[2]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake[1]: *** [CMakeFiles/Makefile2:4720: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2\r\nmake[1]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake: *** [Makefile:139: all] Error 2\r\n```\r\n\r\nThe problem seems to be that Python tries to invoke the `flatbuffers/scripts/generate_code.py` script, which attempts to run `/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc`, which does not exist. \r\n\r\nTFLite v2.10.1 builds fine, but as far as I can tell, for v2.10.1m there is no attempt to invoke `generate_code.py` or `_deps/flatbuffers-build/flatc`\r\n\r\nBased on [https://www.tensorflow.org/lite/guide/build_cmake#specifics_of_kernel_unit_tests_cross-compilation](url), I also tried building flatc separately, but I get the following CMake warning when running: `mkdir flatc-native-build && cd flatc-native-build && cmake ../tensorflow/tensorflow/lite/tools/cmake/native_tools/flatbuffers`\r\n\r\n```\r\nCMake Warning at CMakeLists.txt:43 (find_package):\r\n  By not providing \"Findflatbuffers.cmake\" in CMAKE_MODULE_PATH this project\r\n  has asked CMake to find a package configuration file provided by\r\n  \"flatbuffers\", but CMake did not find one.\r\n\r\n  Could not find a package configuration file provided by \"flatbuffers\" with\r\n  any of the following names:\r\n\r\n    flatbuffersConfig.cmake\r\n    flatbuffers-config.cmake\r\n\r\n  Add the installation prefix of \"flatbuffers\" to CMAKE_PREFIX_PATH or set\r\n  \"flatbuffers_DIR\" to a directory containing one of the above files.  If\r\n  \"flatbuffers\" provides a separate development package or SDK, be sure it\r\n  has been installed.\r\n```\r\nsubsequently running `cmake --build .` does not build the flatc executable.\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nmkdir tflite.build.android && cd tflite.build.android\r\n\r\ncmake -G \"Unix Makefiles\" -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DANDROID_STL=c++_shared -DANDROID_NATIVE_API_LEVEL=27 -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_TOOLCHAIN_FILE=$HOME/Android/Sdk/ndk/24.0.8215888/build/cmake/android.toolchain.cmake -DTFLITE_ENABLE_XNNPACK=ON -DTFLITE_ENABLE_GPU=ON -DTFLITE_ENABLE_NNAPI=ON -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF ../tensorflow/tensorflow/lite\r\n\r\nmake -j4\r\n```\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n28%] Linking CXX executable flatc\r\ncd /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build && /usr/local/bin/cmake -E cmake_link_script CMakeFiles/flatc.dir/link.txt --verbose=1\r\n/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android27 --sysroot=/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/sysroot -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security -fexceptions -frtti -stdlib=libc++ -O3 -DNDEBUG -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Qunused-arguments -Wl,--no-undefined  -Wl,--gc-sections CMakeFiles/flatc.dir/src/idl_parser.cpp.o CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o CMakeFiles/flatc.dir/src/reflection.cpp.o CMakeFiles/flatc.dir/src/util.cpp.o CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o CMakeFiles/flatc.dir/src/flatc.cpp.o CMakeFiles/flatc.dir/src/flatc_main.cpp.o CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o CMakeFiles/flatc.dir/src/code_generators.cpp.o CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o -o flatc   -latomic -lm \r\nRunning scripts/generate_code.py...\r\ncd /home/myname/myworkingdir/tflite.build.android/flatbuffers && /usr/bin/python3.9 scripts/generate_code.py --flatc /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc --skip-gen-reflection\r\nTraceback (most recent call last):\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 148, in <module>\r\n    flatc(\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 82, in flatc\r\n    result = subprocess.run(cmd, cwd=str(cwd), check=True)\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 505, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nOSError: [Errno 8] Exec format error: '/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc'\r\nmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:566: _deps/flatbuffers-build/flatc] Error 1\r\nmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'\r\nmake[2]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake[1]: *** [CMakeFiles/Makefile2:4720: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2\r\nmake[1]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake: *** [Makefile:139: all] Error 2\r\n```\r\n</details></details>"
        },
        {
            "Timestamp": "2022-11-21T16:47:45Z",
            "EntityIds": [
                105635332,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KBBFIK4Y0B5U"
            ],
            "Context": "Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter.. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nOthers\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.10\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 18.04\r\n\r\n### Mobile device\r\n\r\narm64-v8a\r\n\r\n### Python version\r\n\r\n3.7\r\n\r\n### Bazel version\r\n\r\n5.3.0\r\n\r\n### GCC/Compiler version\r\n\r\n7.5\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nGot an error \"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter. Make sure you apply/link the Flex delegate before inference. For the Android, it can be resolved by adding \"org.tensorflow:tensorflow-lite-select-tf-ops\" dependency. See instructions: https://www.tensorflow.org/lite/guide/ops_select. Node number 83 (FlexErf) failed to prepare.\"\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nHi, I try to make inference with flex delegate supported tflite model in Android(arm64-v8a) using C++ language. \r\nI have build tensorflowlite_flex lib and TensorflowLite as below using bazel command on ubuntu system : \r\n'''\r\nbazel build -c opt --cxxopt=--std=c++17 tensorflow/lite/delegates/flex:tensorflowlite_flex --config=monolithic --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\nbazel build -c opt --cxxopt=--std=c++17  //tensorflow/lite:tensorflowlite --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a\r\n'''\r\n\r\nI linked that two library using CMake on Android Studio(Windows 10). \r\nSample CMake snippet:\r\n'''\r\nadd_library(tensorflowlite SHARED IMPORTED)\r\nset_target_properties( tensorflowlite PROPERTIES IMPORTED_LOCATION\r\n                    \"${CMAKE_CURRENT_SOURCE_DIR}/tflite_cpp/lib/arm64-v8a/libtensorflowlite.so\")\r\nadd_library(tensorflowlite_flex SHARED IMPORTED)\r\nset_target_properties( tensorflowlite_flex PROPERTIES IMPORTED_LOCATION\r\n                        \"${CMAKE_CURRENT_SOURCE_DIR}/tflite_cpp/lib/arm64-v8a/libtensorflowlite_flex.so\")\r\ntarget_link_libraries(\r\n        privacy_cpp\r\n        tensorflowlite\r\n        -Wl,--no-as-needed # Need --no-as-needed to link tensorflowlite_flex\r\n        tensorflowlite_flex\r\n        android)\r\n'''\r\n\r\nBut the error log shows \"Select TensorFlow op(s), included in the given model, is(are) not supported by this interpreter.\" What am I doing wrong here? Thank you.\r\n'''\r\nAAsset *model_fp= AAssetManager_open(mgr, \"torch_bert.tflite\", AASSET_MODE_STREAMING );\r\nsize_t size = AAsset_getLength(model_fp);\r\nchar * buf = new char[size];\r\nmemmove(buf, AAsset_getBuffer(model_fp), size);\r\nstd::unique_ptr<tflite::FlatBufferModel> model = tflite::FlatBufferModel::BuildFromBuffer((const char*)buf, size);\r\ntflite::ops::builtin::BuiltinOpResolver resolver;\r\ntflite::InterpreterBuilder(*model, resolver)(&interpreter);\r\n'''\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n_No response_</details>\r\n<img width=\"908\" alt=\"tflite_error\" src=\"https://user-images.githubusercontent.com/105635332/203116570-e4b7a370-1f07-47c3-93c3-3cc47ec7db7b.png\">\r\n\r\n"
        },
        {
            "Timestamp": "2022-11-21T14:03:03Z",
            "EntityIds": [
                74714236,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "LKSIOTIGXVES"
            ],
            "Context": "Wrong invert code in Keras Normalization Layer. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf 2.10\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nThe following code is an excerpt from `class Normalization` of `keras.layers.Normalization`.\r\n\r\n    def call(self, inputs):\r\n        inputs = self._standardize_inputs(inputs)\r\n        # The base layer automatically casts floating-point inputs, but we\r\n        # explicitly cast here to also allow integer inputs to be passed\r\n        inputs = tf.cast(inputs, self.compute_dtype)\r\n        if self.invert:\r\n            return (inputs + self.mean) * tf.maximum(\r\n                tf.sqrt(self.variance), backend.epsilon()\r\n            )\r\n        else:\r\n            return (inputs - self.mean) / tf.maximum(\r\n                tf.sqrt(self.variance), backend.epsilon()\r\n            )\r\n\r\nThe code in the `self.invert == True` branch does not invert the code in the `self.invert == False` branch as it shall according to the API documentation, which states that \"... layer ... would turn a normalized input back into its original form\".\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nThe correct code is:\r\n\r\n    def call(self, inputs):\r\n        inputs = self._standardize_inputs(inputs)\r\n        # The base layer automatically casts floating-point inputs, but we\r\n        # explicitly cast here to also allow integer inputs to be passed\r\n        inputs = tf.cast(inputs, self.compute_dtype)\r\n        if self.invert:\r\n            return self.mean + inputs * tf.maximum(\r\n                tf.sqrt(self.variance), backend.epsilon()\r\n            )\r\n        else:\r\n            return (inputs - self.mean) / tf.maximum(\r\n                tf.sqrt(self.variance), backend.epsilon()\r\n            )\n```\n\n\n### Relevant log output\n\n_No response_</details>"
        },
        {
            "Timestamp": "2022-11-21T12:00:10Z",
            "EntityIds": [
                96806369,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "U5OZR8BECO8X"
            ],
            "Context": "TensorFlow Lite Quantization Debugger Issue. ### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04.3 \r\n- TensorFlow library (version, if pip package or github SHA, if built from source):tensorflow-gpu==2.10.0\r\n\r\n### 2. Code\r\n\r\ndef dense_block(inputs, filters):\r\n\r\n    y = Dense(units=filters,use_bias=False)(inputs)\r\n    y = BatchNormalization()(y)\r\n    return y\r\n\r\ndef model():\r\n    inputs = Input(shape = (112, 112, 3))\r\n    .\r\n    .\r\n    .\r\n    .\r\n    .\r\n    .\r\n    .\r\n    x = dense_block(x, 1)  -> (input_size=1,1,1,256, output_size=1,1,1,1)\r\n    return Model(inputs, x)\r\n\r\n### 3. conversion is successful,but the predicted value of int8 tflite has a large error value with the .pb weights file\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\n- Model produces wrong results and accuracy drop 7~15%.\r\n\r\n### 5. (optional) Any other info / logs\r\n\r\nQuestion1:\r\nMy weights .pb file is successfully quantized into an int8 tflite model.\r\nWhen doing tf.lite.experimental.QuantizationDebugger, the rmse/scale value of the last layer(Conv2D) is 2, which is far more than 0.289, but the rmse/scale values \u200b\u200bin other layers are all It is around 0.289, and if I change the output of the last layer to more nodes, the value of rmse/scale of the last layer will be closer to 0.289.\r\n\r\nDoes anyone know what could be causing this to happen?\r\nthanks!"
        },
        {
            "Timestamp": "2022-11-21T11:20:11Z",
            "EntityIds": [
                913790,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "ZLNCXBUAHKM3"
            ],
            "Context": "[ROCm] Fixed gpu_kernel_tiling_test on RowReductionCorrectShmemUsage. Due to LLVM IR's changes, this test is broken and it needs to change accoridingly on ROCm side.\r\n\r\nThanks   /cc @cheshire "
        },
        {
            "Timestamp": "2022-11-21T11:20:11Z",
            "Entity Ids": [
                913790,
                48215717,
                1835471
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "ZLNCXBUAHKM3",
                "100U6QTCS442"
            ],
            "Context": "[ROCm] Fixed gpu_kernel_tiling_test on RowReductionCorrectShmemUsage. Due to LLVM IR's changes, this test is broken and it needs to change accoridingly on ROCm side.\r\n\r\nThanks   /cc @cheshire "
        },
        {
            "Timestamp": "2022-11-21T10:13:28Z",
            "EntityIds": [
                92745698,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "EYF0EOQN3Q8U"
            ],
            "Context": "Spam. spam removed"
        },
        {
            "Timestamp": "2022-11-21T09:40:36Z",
            "EntityIds": [
                24700291,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KXGE3P5K76RL"
            ],
            "Context": "Bazel build failure. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.4.1\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 18.04.6 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.6.9\r\n\r\n### Bazel version\r\n\r\n3.1.0\r\n\r\n### GCC/Compiler version\r\n\r\n7.5.0\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\nJetson Nano, 32GB of memory\r\n\r\n### Current Behaviour?\r\n\r\nCompiling Tensorflow fails at the end of the process without being able to compile the pooling_ops_gpu file. I am trying to compile Tensorflow in order to enable the tflite_with_xnnpack flag\r\n\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nThis is the Tensorflow configuration pre-build \r\n\r\nYou have bazel 3.1.0- (@non-git) installed.\r\nPlease specify the location of python. [Default is /usr/bin/python3]: /usr/bin/python3\r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/lib/python3/dist-packages\r\n  /usr/local/lib/python3.6/dist-packages\r\n  /usr/lib/python3.6/dist-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]: /usr/lib/python3/dist-packages\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]: n\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with TensorRT support? [y/N]: y\r\nTensorRT support will be enabled for TensorFlow.\r\n\r\nFound CUDA 10.2 in:\r\n    /usr/local/cuda-10.2/targets/aarch64-linux/lib\r\n    /usr/local/cuda-10.2/targets/aarch64-linux/include\r\nFound cuDNN 8 in:\r\n    /usr/lib/aarch64-linux-gnu\r\n    /usr/include\r\nFound TensorRT 7 in:\r\n    /usr/lib/aarch64-linux-gnu\r\n    /usr/include/aarch64-linux-gnu\r\n\r\n\r\nPlease specify a list of comma-separated CUDA compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as \"x.y\" or \"compute_xy\" to include both virtual and binary GPU code, or as \"sm_xy\" to only include the binary code.\r\nPlease note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.3\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: n\r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc\r\n\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -Wno-sign-compare]: -Wno-sign-compare\r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n\r\n\r\nAnd this is the bezel build command used \r\n\r\nsudo bazel --host_jvm_args=-Xmx3278m build --config=opt --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=monolithic --config=cuda --config=v2 --local_cpu_resources=1 --define=tflite_pip_with_flex=true --copt=-ftree-vectorize --copt=-funsafe-math-optimizations --copt=-ftree-loop-vectorize --copt=-fomit-frame-pointer --define tflite_with_xnnpack=true //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nERROR: /home/user/tensorflow/tensorflow/core/kernels/BUILD:4193:1: C++ compilation of rule '//tensorflow/core/kernels:pooling_ops_gpu' failed (Exit 4)\r\nTarget //tensorflow/tools/pip_package:build_pip_package failed to build\r\nUse --verbose_failures to see the command lines of failed build steps.\r\nERROR: /home/user/tensorflow/tensorflow/python/tools/BUILD:226:1 C++ compilation of rule '//tensorflow/core/kernels:pooling_ops_gpu' failed (Exit 4)\r\nINFO: Elapsed time: 165525.324s, Critical Path: 2097.12s\r\nINFO: 13887 processes: 13887 local.\r\nFAILED: Build did NOT complete successfully\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-21T09:31:58Z",
            "EntityIds": [
                44562106,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "DEZKVX6M94EY"
            ],
            "Context": " Cannot build with CUDA support on Windows.. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nSupport\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\nTF 2.11\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nWindows 11\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n5.3.2\r\n\r\n### GCC/Compiler version\r\n\r\nMSVC 14.34.31933\r\n\r\n### CUDA/cuDNN version\r\n\r\n11.8/8.6\r\n\r\n### GPU model and memory\r\n\r\nRTX3090 24GB\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nWhen I'm trying to build the latest stable version TensorFlow-GPU (2.11.0) with CUDA on windows as same as we used to do, I find the following Errors\r\n\r\n`WARNING: Cannot build with CUDA support on Windows. Starting in TF 2.11, CUDA build is not supported for Windows. To use TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2.`\r\n\r\nThis error prevents me to set further CUDA and GPU info, as the consequence, I cannot build GPU version but only CPU version.\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nAs a normal compiling procedure:\r\n\r\n\r\npython .\\configure.py\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nYou have bazel 5.3.2- (@non-git) installed.\r\nPlease specify the location of python. [Default is C:\\Users\\<username>\\miniconda3\\envs\\compile\\python.exe]:\r\n\r\nFound possible Python library paths:\r\n  C:\\Users\\<username>\\miniconda3\\envs\\compile\\Lib\\site-packages\r\nPlease input the desired Python library path to use.  Default is [C:\\Users\\<username>\\miniconda3\\envs\\compile\\Lib\\site-packages]\r\n\r\nDo you wish to build TensorFlow with ROCm support? [y/N]:\r\nNo ROCm support will be enabled for TensorFlow.\r\n\r\nWARNING: Cannot build with CUDA support on Windows.\r\nStarting in TF 2.11, CUDA build is not supported for Windows. For using TensorFlow GPU on Windows, you will need to build/install TensorFlow in WSL2.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is /arch:AVX]:\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-21T06:35:21Z",
            "EntityIds": [
                57779173,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "V2OH5XAG4ZJX"
            ],
            "Context": "tf.distribute.experimental.rpc.Server memory leak on server. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf 2.11\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n11.2/8.1\n\n### GPU model and memory\n\nNvidia GeForce RTX 2070 6233MB\n\n### Current Behaviour?\n\n```shell\nRPC works fine with CPU but when switched to GPU, memory increases linearly.\r\n(Memory stays constant after awhile with CPU)\r\n\r\nWhen training model, memory will keep increasing until desktop freezes.\r\n\r\nCode below to reproduce the issue on Google Colab.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\n# import os\r\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport portpicker\r\nimport matplotlib.pyplot as plt\r\n\r\nimport time\r\nimport os\r\nimport psutil\r\nprocess = psutil.Process(os.getpid())\r\n\r\n\r\n#tf.debugging.set_log_device_placement(True)\r\n\r\n\r\ndef test_simple():\r\n\r\n    @tf.function(input_signature=[\r\n        tf.TensorSpec([], tf.int32),\r\n        tf.TensorSpec([], tf.int32)])\r\n    @tf.autograph.experimental.do_not_convert  \r\n    def remote_fn(a, b):\r\n        return tf.add(a,b)\r\n\r\n    port = portpicker.pick_unused_port()\r\n    address = \"localhost:{}\".format(port)\r\n    server = tf.distribute.experimental.rpc.Server.create(\"grpc\", address)\r\n    server.register(\"addition\", remote_fn)\r\n    server.start()\r\n\r\n    client = tf.distribute.experimental.rpc.Client.create(\"grpc\",\r\n                address=address, name=\"test_client\")\r\n\r\n    a = np.ones(shape=[100, 256], dtype=np.int32)\r\n    b = np.ones(shape=[100, 256], dtype=np.int32)\r\n\r\n    hist = []\r\n    for i in range(100000):\r\n        result = client.call(\r\n            args=[a,b],\r\n            method_name=\"addition\",\r\n            output_specs=tf.TensorSpec((), tf.int32))\r\n        if result.is_ok():\r\n            result.get_value()\r\n        # value = client.addition_blocking(a,b)\r\n        # print(value)\r\n\r\n        if i % 1000 == 0:\r\n            print(i, \": \", process.memory_info().rss/1024/1024, \"MB\")\r\n            hist.append(process.memory_info().rss/1024/1024)\r\n\r\n    server.close()\r\n    print(\"After shutdown: \", process.memory_info().rss/1024/1024)\r\n    plt.plot(hist)\r\n    plt.show()\r\n\r\n\r\ntest_simple()\n```\n\n\n### Relevant log output\n\n```shell\n909000 :  1414.50390625 MB\r\n910000 :  1414.5078125 MB\r\n911000 :  1414.5078125 MB\r\n912000 :  1414.5078125 MB\r\n913000 :  1414.5078125 MB\r\n914000 :  1414.5078125 MB\r\n915000 :  1414.51171875 MB\r\n916000 :  1414.51171875 MB\r\n917000 :  1414.51171875 MB\r\n918000 :  1414.515625 MB\r\n919000 :  1415.6640625 MB\r\n920000 :  1416.796875 MB\r\n921000 :  1416.796875 MB\r\n922000 :  1416.796875 MB\r\n923000 :  1417.75 MB\r\n924000 :  1418.7109375 MB\r\n925000 :  1418.7109375 MB\r\n926000 :  1418.7109375 MB\r\n927000 :  1418.7109375 MB\r\n928000 :  1418.71484375 MB\r\n929000 :  1418.84765625 MB\r\n930000 :  1418.84765625 MB\r\n931000 :  1418.8515625 MB\r\n932000 :  1418.85546875 MB\r\n933000 :  1419.62890625 MB\r\n934000 :  1419.87890625 MB\r\n935000 :  1420.37890625 MB\r\n936000 :  1420.53125 MB\r\n937000 :  1420.65625 MB\r\n938000 :  1421.83984375 MB\r\n939000 :  1421.84375 MB\r\n940000 :  1422.74609375 MB\r\n941000 :  1422.84375 MB\r\n942000 :  1422.84375 MB\r\n943000 :  1423.3515625 MB\r\n944000 :  1423.8984375 MB\r\n945000 :  1423.91015625 MB\r\n946000 :  1425.5078125 MB\r\n947000 :  1426.109375 MB\r\n948000 :  1426.34765625 MB\r\n949000 :  1426.34765625 MB\r\n950000 :  1426.3515625 MB\r\n951000 :  1426.3515625 MB\r\n952000 :  1427.05859375 MB\r\n953000 :  1427.37890625 MB\r\n954000 :  1427.3828125 MB\r\n955000 :  1428.90234375 MB\r\n956000 :  1429.69140625 MB\r\n957000 :  1429.6953125 MB\r\n958000 :  1430.3046875 MB\r\n959000 :  1432.421875 MB\r\n960000 :  1432.421875 MB\r\n961000 :  1432.421875 MB\r\n962000 :  1432.82421875 MB\r\n963000 :  1432.82421875 MB\r\n964000 :  1432.82421875 MB\r\n965000 :  1433.1328125 MB\r\n966000 :  1433.1328125 MB\r\n967000 :  1433.1328125 MB\r\n968000 :  1433.1328125 MB\r\n969000 :  1434.60546875 MB\r\n970000 :  1434.60546875 MB\r\n971000 :  1434.640625 MB\r\n972000 :  1435.08203125 MB\r\n973000 :  1435.0859375 MB\r\n974000 :  1435.59765625 MB\r\n975000 :  1436.98828125 MB\r\n976000 :  1436.9921875 MB\r\n977000 :  1437.125 MB\r\n978000 :  1437.98828125 MB\r\n979000 :  1441.01953125 MB\r\n980000 :  1441.96875 MB\r\n981000 :  1441.96875 MB\r\n982000 :  1441.97265625 MB\r\n983000 :  1441.97265625 MB\r\n984000 :  1441.97265625 MB\r\n985000 :  1441.97265625 MB\r\n986000 :  1441.97265625 MB\r\n987000 :  1441.97265625 MB\r\n988000 :  1441.97265625 MB\r\n989000 :  1441.97265625 MB\r\n990000 :  1441.9765625 MB\r\n991000 :  1441.9765625 MB\r\n992000 :  1441.98046875 MB\r\n993000 :  1444.41796875 MB\r\n994000 :  1444.78515625 MB\r\n995000 :  1444.78515625 MB\r\n996000 :  1444.78515625 MB\r\n997000 :  1446.05078125 MB\r\n998000 :  1446.05078125 MB\r\n999000 :  1446.05078125 MB\r\nAfter shutdown:  1447.05078125\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-21T05:30:01Z",
            "EntityIds": [
                100826172,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KAPF6N2ZKLXX"
            ],
            "Context": "Failed to load the native TensorFlow runtime.. Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**:\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**:\r\n-   **TensorFlow version (use command below)**:\r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"
        },
        {
            "Timestamp": "2022-11-21T00:36:18Z",
            "EntityIds": [
                15100009,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "OZIH452XAYKZ"
            ],
            "Context": "[XLA] Expose channel_id and use_global_device_ids of AllToAll in xla builder. Similar to AllReduce and AllGather, AllToAll also supports `channel_id` and `use_global_device_ids`, but these options are not exposed to the python API.\r\nThis PR exposes these options to the python API of XlaBuilder, following the same logic as AllReduce.\r\n"
        },
        {
            "Timestamp": "2022-11-21T00:36:18Z",
            "Entity Ids": [
                15100009,
                48215717,
                348959,
                1835471
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "OZIH452XAYKZ",
                "WEJDKGZWN9UO"
            ],
            "Context": "[XLA] Expose channel_id and use_global_device_ids of AllToAll in xla builder. Similar to AllReduce and AllGather, AllToAll also supports `channel_id` and `use_global_device_ids`, but these options are not exposed to the python API.\r\nThis PR exposes these options to the python API of XlaBuilder, following the same logic as AllReduce.\r\n"
        },
        {
            "Timestamp": "2022-11-21T00:19:21Z",
            "EntityIds": [
                15100009,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "XJCFNWY6X2AC"
            ],
            "Context": "[FIX] Fix integer overflow in XLA nccl thunks (cont.). Change `int` to `int64_t` to prevent integer overflow in XLA NCCL thunks.\r\n\r\nThis PR is the same as #57616. I am sorry that I didn't fix all of them in the first PR. Now I checked all types of `element_count` and confirmed all of them are fixed."
        },
        {
            "Timestamp": "2022-11-21T00:19:21Z",
            "Entity Ids": [
                15100009,
                48215717,
                348959,
                1835471
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "XJCFNWY6X2AC",
                "YXNLSQTSBQSJ"
            ],
            "Context": "[FIX] Fix integer overflow in XLA nccl thunks (cont.). Change `int` to `int64_t` to prevent integer overflow in XLA NCCL thunks.\r\n\r\nThis PR is the same as #57616. I am sorry that I didn't fix all of them in the first PR. Now I checked all types of `element_count` and confirmed all of them are fixed."
        },
        {
            "Timestamp": "2022-11-20T18:44:28Z",
            "EntityIds": [
                2053858,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "1OPJYU87E5HS"
            ],
            "Context": "Nesting keras.Models makes tf.gradients() not update while training. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.10\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nLinux Ubuntu 20.04\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.8\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nNesting a keras.model  inside another one makes analytic gradients computed with tf.gradients() not update with training. On the toy example presented bellow, the inside network consists of a single multiplication with a trainable variable (i.e. Dense(1, use_bias=False)), computes derivative, and concatenas all 3 so  for input [x] output should be [x,a*x,a] (a is the traibale variable). Training this independently works as expected. \r\nBut if you nest that inside another keras.model(), even a trivial one that multiplies *2, devides /2 (i.e. no change) and then calls the inside model, the gradient computation is unchanged after training. so after you train, the output is [x,a*x,a0] where a0 is the initial weight (1.5 vs 2 in my toy example). \r\nComputing gradients with tf.GradientTape as in here also has the exact same behaviour  https://www.tensorflow.org/guide/advanced_autodiff\r\n\r\nMy 'real world' project involved solving a differential equation using https://github.com/titu1994/tf_SIREN  and switching bewteen training inside and outside model. As gradients dont work properly this way, my workaround was to inline the code of inside model to outside, keep 2 model instances and manual copy weights from one to the other which feels very hacky and I am sure is not indeded.\r\n\r\nTested on tf 2.6,2.8,2.10 , both Ubuntu/Windows and the behaviour is identical everywhere.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport numpy as np\r\nimport tensorflow as tf\r\ntf.random.set_seed(0)\r\n\r\nfrom tensorflow.keras.optimizers import Adam,SGD\r\nfrom tensorflow.keras import Input, Model,Sequential\r\nfrom tensorflow.keras.initializers import Constant\r\nfrom tensorflow.keras.layers import Layer, Dense\r\nfrom tensorflow.keras.layers import Concatenate, concatenate\r\n\r\n\r\ntf.compat.v1.disable_eager_execution()\r\n\r\n#trivial network doing multiplication by a single trainable variable\r\ndef inside_network():\r\n    x = Input(1)    \r\n        \r\n    layer=Dense(1,activation='linear',use_bias=False, kernel_initializer=Constant(value=1.5)) \r\n    y=layer(x)    \r\n\r\n    dy=tf.gradients(y,[x])[0]  \r\n    #for weight a, output should be [x, ax, a]\r\n    out= concatenate([x, y, dy],axis=-1)\r\n    \r\n    network = Model(inputs = x, outputs = out, name='inside_network')\r\n    \r\n    return network\r\n\r\ndef outside_network():\r\n    x = Input((1))      \r\n    y=2*x\r\n    y=y*0.5\r\n    model=inside_network()\r\n    y=model(y)\r\n    \r\n    outside_network = Model(inputs = x, outputs = y, name='outside_network')\r\n    return outside_network\r\n    \r\ndef get_first_weights(mod):\r\n    for layer in mod.layers:\r\n        w=layer.get_weights()\r\n        if len(w)>0:\r\n            return w\r\n\r\nNdata=1024\r\nnp.random.seed(0)\r\nxdata=np.random.rand(Ndata,1)\r\nxdata[0:4,0]=np.arange(0,4)\r\na=2.0\r\n\r\ndata_train_target=np.concatenate([xdata, a*xdata, a*np.ones((Ndata,1))],axis=-1)\r\nprint('input :4',xdata[:4,:])\r\nprint('target :4',data_train_target[:4,:])\r\n\r\nmodel1=inside_network()\r\n\r\nmodel1.compile(optimizer=Adam(1e-2),  loss='mae') #\r\nprint('dense layer weights before training',get_first_weights(model1))\r\n\r\nmodel1.fit(xdata,data_train_target,verbose=1,epochs=2,shuffle=True) #,batch_size=bs\r\n   \r\nprint('dense layer weights after training',get_first_weights(model1))\r\n           \r\ndataout=model1.predict(xdata)\r\nprint('dataout',dataout[0:4])\r\nprint('target',data_train_target[0:4,:])\r\n\r\nmodel2=outside_network()\r\n\r\nmodel2.compile(optimizer=Adam(1e-2),  loss='mae') #\r\n\r\nprint('dense layer weights before training',get_first_weights(model2))\r\n\r\n\r\n\r\nmodel2.fit(xdata,data_train_target,verbose=1,epochs=2,shuffle=True)\r\n\r\nprint('dense layer weights after training',get_first_weights(model2))\r\n\r\ndataout2=model2.predict(xdata)\r\nprint('dataout2',dataout2[0:4])\r\nprint(data_train_target[:4,:])\n```\n\n\n### Relevant log output\n\n```shell\ninput :4 [[0.]\r\n [1.]\r\n [2.]\r\n [3.]]\r\ntarget :4 [[0. 0. 2.]\r\n [1. 2. 2.]\r\n [2. 4. 2.]\r\n [3. 6. 2.]]\r\n2022-11-20 18:27:50.249131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:50.295560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:50.295870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:50.298148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:50.298388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:50.298603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:51.014096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:51.014341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:51.014531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2022-11-20 18:27:51.014710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8946 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5\r\ndense layer weights before training [array([[1.5]], dtype=float32)]\r\nTrain on 1024 samples\r\nEpoch 1/2\r\n1024/1024 [==============================] - 1s 927us/sample - loss: 0.1731\r\nEpoch 2/2\r\n1024/1024 [==============================] - 0s 43us/sample - loss: 0.0320\r\ndense layer weights after training [array([[2.011962]], dtype=float32)]\r\n/home/flogothetis/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\r\n  updates=self.state_updates,\r\ndataout [[0.       0.       2.011962]\r\n [1.       2.011962 2.011962]\r\n [2.       4.023924 2.011962]\r\n [3.       6.035886 2.011962]]\r\ntarget [[0. 0. 2.]\r\n [1. 2. 2.]\r\n [2. 4. 2.]\r\n [3. 6. 2.]]\r\ndense layer weights before training [array([[1.5]], dtype=float32)]\r\nTrain on 1024 samples\r\nEpoch 1/2\r\n1024/1024 [==============================] - 0s 142us/sample - loss: 0.2240\r\nEpoch 2/2\r\n1024/1024 [==============================] - 0s 58us/sample - loss: 0.1774\r\ndense layer weights after training [array([[2.0133402]], dtype=float32)]\r\ndataout2 [[0.        0.        1.5      ]\r\n [1.        2.0133402 1.5      ]\r\n [2.        4.0266805 1.5      ]\r\n [3.        6.040021  1.5      ]]\r\n[[0. 0. 2.]\r\n [1. 2. 2.]\r\n [2. 4. 2.]\r\n [3. 6. 2.]]\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-20T16:15:50Z",
            "EntityIds": [
                38817737,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "ZD9PV5QB9QA5"
            ],
            "Context": "Android 12 TensorFlow ML operations are not supported by GPU delegate. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf 2.9\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nAndroid 12 (API 31)\n\n### Mobile device\n\nGoogle Pixel 4\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nTensorflow lite pose estimation Crashed when switched to GPU.\r\nIt says some TF operations are not supported by GPU delegate.\r\nInterestingly when the API is 30 (Android 11) the app runs fine when changing to GPU CPU or NNAPI. No matter what tf model, Movenet lightning or thunder, int 8 or float 16.\r\n\r\nI tried it on the TF lite example post estimation APP, changed the compile and target sdk to 31, it crashed with the same error.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nfun create(context: Context, device: Device, modelType: ModelType): MoveNet {\r\n            val options = Interpreter.Options()\r\n            var gpuDelegate: GpuDelegate? = null\r\n            options.setNumThreads(CPU_NUM_THREADS)\r\n            when (device) {\r\n                Device.CPU -> {\r\n                }\r\n                Device.GPU -> {\r\n                    gpuDelegate = GpuDelegate()\r\n                    options.addDelegate(gpuDelegate)\r\n                }\r\n                Device.NNAPI -> options.setUseNNAPI(true)\r\n            }\r\n            return MoveNet(\r\n                Interpreter(\r\n                    FileUtil.loadMappedFile(\r\n                        context,\r\n                        if (modelType == ModelType.Lightning) LIGHTNING_FILENAME\r\n                        else THUNDER_FILENAME\r\n                    ), options\r\n                ),\r\n                gpuDelegate\r\n            )\r\n        }\n```\n\n\n### Relevant log output\n\n```shell\njava.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Following operations are not supported by GPU delegate:\r\n    ARG_MAX: Operation is not supported.\r\n    CAST: Not supported cast case\r\n    CONCATENATION: OP is supported, but tensor type isn't matched!\r\n    FLOOR_DIV: OP is supported, but tensor type isn't matched!\r\n    GATHER_ND: Operation is not supported.\r\n    MUL: OP is supported, but tensor type isn't matched!\r\n    PACK: OP is supported, but tensor type isn't matched!\r\n    RESHAPE: OP is supported, but tensor type isn't matched!\r\n    SUB: OP\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-20T15:45:02Z",
            "EntityIds": [
                26105224,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "LWB3477VN4W2"
            ],
            "Context": "Tensorflow lite dynamic input. ### 1. System information\r\n\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nWindows 11\r\n- TensorFlow installation (pip package or built from source):\r\npip 2.10\r\n\r\n\r\n### 2. Code\r\ninputs = tf.keras.layers.Input(shape=(None, None, 3), name='input_image')\r\nmodel = tf.keras.Model(inputs, inputs)\r\n\r\ninput_image - [(None, None, None,  3)]\r\n\r\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\r\ntflite_model = converter.convert()\r\n\r\ninput_image - [1 1 1 3]"
        },
        {
            "Timestamp": "2022-11-20T13:16:10Z",
            "EntityIds": [
                17670772,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "1UM4IWWHASZ3"
            ],
            "Context": "Invalid argument in example. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBug\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.11\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux Ubuntu 22.04.1\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.10\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n11.2\r\n\r\n### GPU model and memory\r\n\r\nRTX 3090 24GB\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nThis example returns invalid argument message, but it still runs. I wonder if that is a warning or error.\r\n\r\nhttps://www.tensorflow.org/tutorials/images/classification\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\nhttps://www.tensorflow.org/tutorials/images/classification\r\n\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nlayout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-20T03:29:04Z",
            "EntityIds": [
                22030060,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "K6J05OR86J99"
            ],
            "Context": "TF 2.11 Error: ModuleNotFoundError: No module named 'tensorflow.compat'. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.11\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nWindows 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.10.04\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nI tried training my model that used the functional API and got this error:\r\n\r\nModuleNotFoundError: No module named 'tensorflow.compat'\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nDon't have it.\n```\n\n\n### Relevant log output\n\n```shell\nfrom keras.layers import MaxPooling1D\r\n  File \"C:\\Development\\Python\\Python3104\\lib\\site-packages\\keras\\__init__.py\", line 21, in <module>\r\n    from keras import models\r\n  File \"C:\\Development\\Python\\Python3104\\lib\\site-packages\\keras\\models\\__init__.py\", line 18, in <module>\r\n    from keras.engine.functional import Functional\r\n  File \"C:\\Development\\Python\\Python3104\\lib\\site-packages\\keras\\engine\\functional.py\", line 24, in <module>\r\n    import tensorflow.compat.v2 as tf\r\nModuleNotFoundError: No module named 'tensorflow.compat'\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-19T16:34:26Z",
            "EntityIds": [
                53497039,
                84765720
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "IN1OHTNQLPY9"
            ],
            "Context": "The experimental lcm function gives wrong result. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.9.2\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nLinux (Google Colab)\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.15\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nHi. The code below shows the lcm for 21 and 7 is output as 15 for TensorFlow while it should clearly be 21. NumPy and JAX give the correct result of 21. This is the case for int8 although the output value is certainly not high enough to be causing any overflows etc. The result in case of int16 or higher is accurate (21). However, if the values were 2100 and 70 the result would be inaccurate for int16 in tensorflow again as opposed to np and jax.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nat = tf.constant([21], dtype=tf.int8)\r\nbt = tf.constant([7], dtype=tf.int8)\r\n\r\nan = np.array([21], dtype=np.int8)\r\nbn = np.array([7], dtype=jnp.int8)\r\n\r\nprint(\"tensorflow: \", tf.experimental.numpy.lcm(at, bt))\r\nprint(\"numpy: \", np.lcm(an,bn))\n```\n\n\n### Relevant log output\n\n```shell\ntensorflow:  tf.Tensor([15], shape=(1,), dtype=int8)\r\nnumpy:  [21]\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-19T16:28:39Z",
            "EntityIds": [
                2458806,
                81610181
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "O088054RCCK3"
            ],
            "Context": "Compiling libtensorflow for ios issues : can't fine Eigen/Core. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBuild/Install\n\n### Source\n\nsource\n\n### Tensorflow Version\n\ntf 2.10\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nMacOs ventura 13.0\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n5.3.2\n\n### GCC/Compiler version\n\nclang 14.0 (Xcode shipped)\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nHello !\r\n\r\nI am not able to build libtensorflow.so for ios.\r\nI just `./configure` and accepted all defaults.\r\n\r\nI can build for my mac:\r\n`bazel build -c opt //tensorflow:libtensorflow.so`\r\nbut cross compiling for ios\r\n`bazel build -c opt --config=ios //tensorflow:libtensorflow.so`\r\n\r\ngives me this error\r\n\r\n`\r\nERROR: /Users/SX/Documents/soundx-ai/vendor/tensorflow/tensorflow/cc/saved_model/BUILD:319:11: Compiling tensorflow/cc/saved_model/metrics.cc failed: (Aborted): wrapped_clang_pp failed: error executing command external/local_config_cc/wrapped_clang_pp '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 -DNDEBUG ... (remaining 32 arguments skipped)\r\nIn file included from tensorflow/cc/saved_model/metrics.cc:16:\r\nIn file included from ./tensorflow/cc/saved_model/metrics.h:25:\r\nIn file included from ./tensorflow/core/lib/monitoring/counter.h:19:\r\nIn file included from ./tensorflow/tsl/lib/monitoring/counter.h:85:\r\nIn file included from ./tensorflow/tsl/lib/monitoring/collection_registry.h:109:\r\nIn file included from ./tensorflow/tsl/lib/monitoring/collected_metrics.h:28:\r\nIn file included from ./tensorflow/tsl/lib/monitoring/metric_def.h:24:\r\nIn file included from ./tensorflow/tsl/lib/monitoring/types.h:22:\r\nIn file included from ./tensorflow/tsl/platform/types.h:21:\r\nIn file included from ./tensorflow/tsl/platform/bfloat16.h:20:\r\n./third_party/eigen3/Eigen/Core:1:10: fatal error: 'Eigen/Core' file not found\r\n#include \"Eigen/Core\"\r\n```\r\n\r\nI have a eigen3 lib installed with brew, in my `/opt/homebrew/Cellar/eigen/3.4.0_1/include/eigen3/Eigen/Core`, but I did not get how I could give this information to bazel. I'm not able to play with the path directly.\r\n\r\nAny clue to solve this problem ?\r\n\r\nThank you!\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nbazel build -c opt --config=ios //tensorflow:libtensorflow.so\n```\n\n\n### Relevant log output\n\n_No response_</details>"
        },
        {
            "Timestamp": "2022-11-19T12:53:43Z",
            "EntityIds": [
                33950866,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "26RBSE3IEGSC"
            ],
            "Context": "the priority of H2D. Question\uff1ahttps://github.com/tensorflow/tensorflow/issues/58616\r\nModify:\r\nWe distinguish FeaturesAsyncH2D and ComputeH2D, and raise the priority of ComputeH2D and lower the priority of FeaturesAsyncH2D.\r\nResult:\r\n<img width=\"1678\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33950866/202851818-03031f51-a0df-4feb-a6b1-30203096a523.png\">\r\n\r\n**Due to busy work, if you find this feature useful, I will complete my PR later.**\r\n\r\n"
        },
        {
            "Timestamp": "2022-11-19T12:53:43Z",
            "Entity Ids": [
                33950866,
                48215717,
                9094048
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "26RBSE3IEGSC",
                "T25MA86H97OY"
            ],
            "Context": "the priority of H2D. Question\uff1ahttps://github.com/tensorflow/tensorflow/issues/58616\r\nModify:\r\nWe distinguish FeaturesAsyncH2D and ComputeH2D, and raise the priority of ComputeH2D and lower the priority of FeaturesAsyncH2D.\r\nResult:\r\n<img width=\"1678\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33950866/202851818-03031f51-a0df-4feb-a6b1-30203096a523.png\">\r\n\r\n**Due to busy work, if you find this feature useful, I will complete my PR later.**\r\n\r\n"
        },
        {
            "Timestamp": "2022-11-19T12:36:22Z",
            "EntityIds": [
                33950866,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KWHNJDLK317H"
            ],
            "Context": "ComputeH2D is more priority than FeaturesH2D. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nPerformance\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.8\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux\r\n\r\n### Mobile device\r\n\r\nno\r\n\r\n### Python version\r\n\r\n2.7\r\n\r\n### Bazel version\r\n\r\nmaster\r\n\r\n### GCC/Compiler version\r\n\r\n10.2.0\r\n\r\n### CUDA/cuDNN version\r\n\r\nCUDA Version: 11.4\r\n\r\n### GPU model and memory\r\n\r\nyes\r\n\r\n### Current Behaviour?\r\n\r\n**BackGround 1**: CUDA only support once H2D in same time .\r\n**BackGround 2**: \r\n  1. FeaturesAsyncH2D: TrainingData H2D by `dataset.prefetch_to_device`\r\n  2. ComputeH2D: Some small H2D in Op. for example: DynamicPartitionOpGPU: CPU compute output size, and then transfer size to GPU.\r\n\r\n**BackGround 3**: ComputeH2D maybe block current compute, and FeaturesAsyncH2D is needed to next few steps, so ComputeH2D is more important than FeaturesAsyncH2D.\r\n\r\nWhen lots of features need AsyncH2D, they maybe block ComputeH2D, because TF don't distinguish the priority of ComputeH2D and FeaturesAsyncH2D. \r\nwe use nsys to recurrent this bug:\r\n<img width=\"1560\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33950866/202851210-cb72e589-9dec-4a85-a35b-365b63ace052.png\">\r\n\r\n</details>"
        },
        {
            "Timestamp": "2022-11-19T06:09:14Z",
            "EntityIds": [
                42224728,
                38085909,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "Z4920MTODUGQ"
            ],
            "Context": "ComputeH2D is more priority than FeaturesH2D. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nPerformance\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.8\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nLinux\r\n\r\n### Mobile device\r\n\r\nno\r\n\r\n### Python version\r\n\r\n2.7\r\n\r\n### Bazel version\r\n\r\nmaster\r\n\r\n### GCC/Compiler version\r\n\r\n10.2.0\r\n\r\n### CUDA/cuDNN version\r\n\r\nCUDA Version: 11.4\r\n\r\n### GPU model and memory\r\n\r\nyes\r\n\r\n### Current Behaviour?\r\n\r\n**BackGround 1**: CUDA only support once H2D in same time .\r\n**BackGround 2**: \r\n  1. FeaturesAsyncH2D: TrainingData H2D by `dataset.prefetch_to_device`\r\n  2. ComputeH2D: Some small H2D in Op. for example: DynamicPartitionOpGPU: CPU compute output size, and then transfer size to GPU.\r\n\r\n**BackGround 3**: ComputeH2D maybe block current compute, and FeaturesAsyncH2D is needed to next few steps, so ComputeH2D is more important than FeaturesAsyncH2D.\r\n\r\nWhen lots of features need AsyncH2D, they maybe block ComputeH2D, because TF don't distinguish the priority of ComputeH2D and FeaturesAsyncH2D. \r\nwe use nsys to recurrent this bug:\r\n<img width=\"1560\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33950866/202851210-cb72e589-9dec-4a85-a35b-365b63ace052.png\">\r\n\r\n</details>"
        },
        {
            "Timestamp": "2022-11-19T06:09:14Z",
            "Entity Ids": [
                42224728,
                38085909,
                48215717,
                1174378
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "Z4920MTODUGQ",
                "1PODG5N3BZSJ"
            ],
            "Context": "the priority of H2D. Question\uff1ahttps://github.com/tensorflow/tensorflow/issues/58616\r\nModify:\r\nWe distinguish FeaturesAsyncH2D and ComputeH2D, and raise the priority of ComputeH2D and lower the priority of FeaturesAsyncH2D.\r\nResult:\r\n<img width=\"1678\" alt=\"image\" src=\"https://user-images.githubusercontent.com/33950866/202851818-03031f51-a0df-4feb-a6b1-30203096a523.png\">\r\n\r\n**Due to busy work, if you find this feature useful, I will complete my PR later.**\r\n\r\n"
        },
        {
            "Timestamp": "2022-11-18T16:08:12Z",
            "EntityIds": [
                59843233,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "YOKEEMT5Q8MY"
            ],
            "Context": "Gradient return None. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.9.1\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nUbuntu\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nI implement a model with a custom training loop and custom loss function. Loss function return a float value. But while calculate gradient tf.gradient(loss,model.trainable_weights), It gives None gradient.  I know the issue is way of calculate loss. I tried with custom mse loss. It works fine. I want to implement loss function like count_predition_0's+count_predition_1's/label_0's+label_1's. It's a binary classification problem. I set batch size is 100. So model return 100 batch output. I only consider few batch eg out of 100 batch i only consider or filter it out 40 based on input. Both label and prediction in same shape that is not issue here. That label and prediction pass to custom_loss function.\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\ndef custom_loss(self,output,prediction):\r\n        \r\n     correct_count=tf.math.count_nonzero(tf.math.equal(tf.round(prediction),tf.cast(output,tf.float32)))\r\n        incorrect_count=prediction.shape[0]-correct_count\r\n\r\n        incorrect_label_count=tf.where(output==0.0).shape[0]\r\n        correct_label_count=tf.where(output==1.0).shape[0]\r\n            \r\n        return tf.math.divide(tf.math.add(correct_count,incorrect_count),output.shape[0])\n```\n\n\n### Relevant log output\n\n```shell\nValueError: No gradients provided for any variable: (['my_model/object/embeddings:0', 'my_model/dense/kernel:0', 'my_model/dense/bias:0', 'my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0', 'my_model/dense_2/kernel:0', 'my_model/dense_2/bias:0', 'my_model/dense_3/kernel:0', 'my_model/dense_3/bias:0', 'my_model/dense_4/kernel:0', 'my_model/dense_4/bias:0', 'my_model/dense_5/kernel:0', 'my_model/dense_5/bias:0', 'my_model/dense_6/kernel:0', 'my_model/dense_6/bias:0', 'my_model/dense_7/kernel:0', 'my_model/dense_7/bias:0'],). Provided `grads_and_vars` is ((None, <tf.Variable 'my_model/object/embeddings:0' shape=(263, 100) dtype=float32, numpy=\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-18T15:42:56Z",
            "EntityIds": [
                14215174,
                73069040
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "XRI2N0ATH8D2"
            ],
            "Context": "Tensorflow Lite C API works much slower than C++ API. ### Issue Type\r\n\r\nPerformance\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\ntflite v2.4.0\r\n\r\n### Custom Code\r\n\r\nYes\r\n\r\n### OS Platform and Distribution\r\n\r\nUbuntu 18.04.6 LTS\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n_No response_\r\n\r\n### Bazel version\r\n\r\nCMake was used\r\n\r\n### GCC/Compiler version\r\n\r\ngcc 7.5.0\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\n```shell\r\nI built TFLite v2.4.0 static library and used in our project using C++ API \r\n\r\n(we are using it cross-platform Ubuntu/Android/iOS). It shows inference time 5-10 ms on Ubuntu 18. \r\n\r\nWe decided to switch to C API (iOS xcframework is available on C). But I see that on \r\n\r\nC API inference time becomes 45-50 ms. I tried to test using TFLite v2.10.0 C API \r\n\r\nand it shows 180-200 ms. The model tested in Ubuntu is a small OCR model \r\n\r\nwith size smaller than 200 KB.\r\n```\r\n\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nI can't upload the whole files, so here is snippets:\r\n\r\nclass TextRecognizer {\r\n    ...\r\n    virtual TfLiteModel* getModel() = 0;\r\n    TfLiteInterpreterOptions* options;\r\n    TfLiteInterpreter* interpreter;\r\n};\r\n```\r\n\r\nCpp file:\r\n```\r\nvdoc::TextLinePrediction vdoc::TextRecognizer::predict(const cv::Mat &srcImage) {\r\n    ...\r\n    options = TfLiteInterpreterOptionsCreate();\r\n    interpreter = TfLiteInterpreterCreate(getModel(), options);\r\n    TfLiteInterpreterAllocateTensors(interpreter);\r\n    ...\r\n    TfLiteTensor *input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);\r\n    TfLiteTensorCopyFromBuffer(input_tensor, convertedImage.data,\r\n                                   convertedImage.total() * convertedImage.elemSize());\r\n    TfLiteInterpreterInvoke(interpreter);\r\n    const TfLiteTensor *output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 0);\r\n    float *output = output_tensor->data.f;\r\n    ...\r\n}\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\nOutput is measured time of each TFLite method call:\r\nTime taken to init interpreter: 0.00191s\r\nTime taken to allocate tensors: 0.00060s\r\nTime taken to copy buffer data: 0.00031s\r\nTime taken to Invoke inference: 0.05476s\r\n\r\nAs I mentioned before on C++ API inference time is about 10 times faster 0.005s.\r\n```\r\n"
        },
        {
            "Timestamp": "2022-11-18T15:07:02Z",
            "EntityIds": [
                15980055,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "4T170GRB63AV"
            ],
            "Context": "Wrong barplot x-labels in https://www.tensorflow.org/tutorials/audio/simple_audio. I think that in the notebook https://www.tensorflow.org/tutorials/audio/simple_audio this line:\r\n\r\n`plt.bar(commands, tf.nn.softmax(prediction[0]))`\r\n\r\nshould be replaced with:\r\n\r\n`x_labels = ['down', 'go', 'left', 'no', 'right', 'stop', 'up', 'yes']`\r\n`plt.bar(x_labels, tf.nn.softmax(prediction[0]))`\r\n\r\nAs a result, the graph will correctly show the probabilities for all classes. \r\n\r\n"
        },
        {
            "Timestamp": "2022-11-18T13:38:41Z",
            "EntityIds": [
                67419721,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "4Q1DG0PYL6QU"
            ],
            "Context": "Tensorflow Lite 2.11.0-rc2 CMake build system broken. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBuild/Install\r\n\r\n### Source\r\n\r\nsource\r\n\r\n### Tensorflow Version\r\n\r\n2.11.0-rc2\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nAndroid\r\n\r\n### Mobile device\r\n\r\nN/A\r\n\r\n### Python version\r\n\r\n3.9\r\n\r\n### Bazel version\r\n\r\nN/A\r\n\r\n### GCC/Compiler version\r\n\r\nAndroid NDK 24\r\n\r\n### CUDA/cuDNN version\r\n\r\nN/A\r\n\r\n### GPU model and memory\r\n\r\nN/A\r\n\r\n### Current Behaviour?\r\n\r\nUnable to cross compile TFLite 2.11.0-rc2 for Android, using CMake.\r\nI understand that v2.11.0 is still a release candidate. But I want to bring this issue to your attention. We rely heavily on TFLite (and the CMake workflow) in our production and we are looking forward to upgrading to v2.11.0, once it is officially released, to take advantage of fixes to long standing bugs and issues.\r\n\r\nHere is the command I used to build:\r\n\r\n```shell\r\nmkdir tflite.build.android && cd tflite.build.android\r\n\r\ncmake -G \"Unix Makefiles\" -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DANDROID_STL=c++_shared -DANDROID_NATIVE_API_LEVEL=27 -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_TOOLCHAIN_FILE=$HOME/Android/Sdk/ndk/24.0.8215888/build/cmake/android.toolchain.cmake -DTFLITE_ENABLE_XNNPACK=ON -DTFLITE_ENABLE_GPU=ON -DTFLITE_ENABLE_NNAPI=ON -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF ../tensorflow/tensorflow/lite\r\n\r\nmake -j4\r\n```\r\n, and I get the following error:\r\n```\r\n 28%] Linking CXX executable flatc\r\ncd /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build && /usr/local/bin/cmake -E cmake_link_script CMakeFiles/flatc.dir/link.txt --verbose=1\r\n/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android27 --sysroot=/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/sysroot -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security -fexceptions -frtti -stdlib=libc++ -O3 -DNDEBUG -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Qunused-arguments -Wl,--no-undefined  -Wl,--gc-sections CMakeFiles/flatc.dir/src/idl_parser.cpp.o CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o CMakeFiles/flatc.dir/src/reflection.cpp.o CMakeFiles/flatc.dir/src/util.cpp.o CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o CMakeFiles/flatc.dir/src/flatc.cpp.o CMakeFiles/flatc.dir/src/flatc_main.cpp.o CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o CMakeFiles/flatc.dir/src/code_generators.cpp.o CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o -o flatc   -latomic -lm \r\nRunning scripts/generate_code.py...\r\ncd /home/myname/myworkingdir/tflite.build.android/flatbuffers && /usr/bin/python3.9 scripts/generate_code.py --flatc /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc --skip-gen-reflection\r\nTraceback (most recent call last):\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 148, in <module>\r\n    flatc(\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 82, in flatc\r\n    result = subprocess.run(cmd, cwd=str(cwd), check=True)\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 505, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nOSError: [Errno 8] Exec format error: '/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc'\r\nmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:566: _deps/flatbuffers-build/flatc] Error 1\r\nmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'\r\nmake[2]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake[1]: *** [CMakeFiles/Makefile2:4720: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2\r\nmake[1]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake: *** [Makefile:139: all] Error 2\r\n```\r\n\r\nThe problem seems to be that Python tries to invoke the `flatbuffers/scripts/generate_code.py` script, which attempts to run `/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc`, which does not exist. \r\n\r\nTFLite v2.10.1 builds fine, but as far as I can tell, for v2.10.1m there is no attempt to invoke `generate_code.py` or `_deps/flatbuffers-build/flatc`\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\nmkdir tflite.build.android && cd tflite.build.android\r\n\r\ncmake -G \"Unix Makefiles\" -DCMAKE_SYSTEM_NAME=Android -DANDROID_ABI=arm64-v8a -DANDROID_STL=c++_shared -DANDROID_NATIVE_API_LEVEL=27 -DCMAKE_VERBOSE_MAKEFILE=ON -DCMAKE_TOOLCHAIN_FILE=$HOME/Android/Sdk/ndk/24.0.8215888/build/cmake/android.toolchain.cmake -DTFLITE_ENABLE_XNNPACK=ON -DTFLITE_ENABLE_GPU=ON -DTFLITE_ENABLE_NNAPI=ON -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF ../tensorflow/tensorflow/lite\r\n\r\nmake -j4\r\n```\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n28%] Linking CXX executable flatc\r\ncd /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build && /usr/local/bin/cmake -E cmake_link_script CMakeFiles/flatc.dir/link.txt --verbose=1\r\n/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android27 --sysroot=/home/myname/Android/Sdk/ndk/24.0.8215888/toolchains/llvm/prebuilt/linux-x86_64/sysroot -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security -fexceptions -frtti -stdlib=libc++ -O3 -DNDEBUG -Wl,--build-id=sha1 -Wl,--no-rosegment -Wl,--fatal-warnings -Qunused-arguments -Wl,--no-undefined  -Wl,--gc-sections CMakeFiles/flatc.dir/src/idl_parser.cpp.o CMakeFiles/flatc.dir/src/idl_gen_text.cpp.o CMakeFiles/flatc.dir/src/reflection.cpp.o CMakeFiles/flatc.dir/src/util.cpp.o CMakeFiles/flatc.dir/src/idl_gen_cpp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_csharp.cpp.o CMakeFiles/flatc.dir/src/idl_gen_dart.cpp.o CMakeFiles/flatc.dir/src/idl_gen_kotlin.cpp.o CMakeFiles/flatc.dir/src/idl_gen_go.cpp.o CMakeFiles/flatc.dir/src/idl_gen_java.cpp.o CMakeFiles/flatc.dir/src/idl_gen_ts.cpp.o CMakeFiles/flatc.dir/src/idl_gen_php.cpp.o CMakeFiles/flatc.dir/src/idl_gen_python.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lobster.cpp.o CMakeFiles/flatc.dir/src/idl_gen_lua.cpp.o CMakeFiles/flatc.dir/src/idl_gen_rust.cpp.o CMakeFiles/flatc.dir/src/idl_gen_fbs.cpp.o CMakeFiles/flatc.dir/src/idl_gen_grpc.cpp.o CMakeFiles/flatc.dir/src/idl_gen_json_schema.cpp.o CMakeFiles/flatc.dir/src/idl_gen_swift.cpp.o CMakeFiles/flatc.dir/src/flatc.cpp.o CMakeFiles/flatc.dir/src/flatc_main.cpp.o CMakeFiles/flatc.dir/src/bfbs_gen_lua.cpp.o CMakeFiles/flatc.dir/src/code_generators.cpp.o CMakeFiles/flatc.dir/grpc/src/compiler/cpp_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/go_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/java_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/python_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/swift_generator.cc.o CMakeFiles/flatc.dir/grpc/src/compiler/ts_generator.cc.o -o flatc   -latomic -lm \r\nRunning scripts/generate_code.py...\r\ncd /home/myname/myworkingdir/tflite.build.android/flatbuffers && /usr/bin/python3.9 scripts/generate_code.py --flatc /home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc --skip-gen-reflection\r\nTraceback (most recent call last):\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 148, in <module>\r\n    flatc(\r\n  File \"/home/myname/myworkingdir/tflite.build.android/flatbuffers/scripts/generate_code.py\", line 82, in flatc\r\n    result = subprocess.run(cmd, cwd=str(cwd), check=True)\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 505, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/usr/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nOSError: [Errno 8] Exec format error: '/home/myname/myworkingdir/tflite.build.android/_deps/flatbuffers-build/flatc'\r\nmake[2]: *** [_deps/flatbuffers-build/CMakeFiles/flatc.dir/build.make:566: _deps/flatbuffers-build/flatc] Error 1\r\nmake[2]: *** Deleting file '_deps/flatbuffers-build/flatc'\r\nmake[2]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake[1]: *** [CMakeFiles/Makefile2:4720: _deps/flatbuffers-build/CMakeFiles/flatc.dir/all] Error 2\r\nmake[1]: Leaving directory '/home/myname/myworkingdir/tflite.build.android'\r\nmake: *** [Makefile:139: all] Error 2\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-18T12:10:40Z",
            "EntityIds": [
                95025816,
                116063290
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "SOVX7DELWFN6"
            ],
            "Context": "from keras.models import load_model raises no module named tensorflow.compat error. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nSupport\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.11\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nWindoes 11\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.7.9\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nWhen doing  keras.models import load_model, An error saying no module named tensorflow.compat appears\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nJust open python 3.7 and type  keras.models import load_model\n```\n\n\n### Relevant log output\n\n```shell\nTraceback (most recent call last):\r\n  File \"c:/Users/Noah Ryu/Coding/AI/Teachable Machine/Guesser3.py\", line 1, in <module>\r\n    from keras.models import load_model # TensorFlow is needed for Keras to work\r\n  File \"C:\\Users\\Noah Ryu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\__init__.py\", line 21, in <module>\r\n    from keras import models\r\n  File \"C:\\Users\\Noah Ryu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\models\\__init__.py\", line 18, in <module>  \r\n    from keras.engine.functional import Functional\r\n  File \"C:\\Users\\Noah Ryu\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\engine\\functional.py\", line 24, in <module>\r\n    import tensorflow.compat.v2 as tf\r\nModuleNotFoundError: No module named 'tensorflow.compat'\n```\n</details>"
        },
        {
            "Timestamp": "2022-11-18T08:48:08Z",
            "EntityIds": [
                87115287,
                73069040
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "A0KK92PC16WA"
            ],
            "Context": "gpu metal  lack  \u201dinference_context_generated.h\u201c. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nsource\n\n### Tensorflow Version\n\n2.10 or 2.11\n\n### Custom Code\n\nYes\n\n### OS Platform and Distribution\n\nios\n\n### Mobile device\n\niphone\n\n### Python version\n\n3.7\n\n### Bazel version\n\nno \n\n### GCC/Compiler version\n\nno\n\n### CUDA/cuDNN version\n\nno\n\n### GPU model and memory\n\n111\n\n### Current Behaviour?\n\n```shell\nA bug happened!\r\n#include \"tensorflow/lite/delegates/gpu/metal/inference_context_generated.h\"\r\nBut I can not find the inference_context_generated.h.WHy?\r\nThanks\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\n111\n```\n\n\n### Relevant log output\n\n_No response_</details>"
        },
        {
            "Timestamp": "2022-11-18T00:23:19Z",
            "EntityIds": [
                12981474,
                144114,
                6878204,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "0JO6G690EJJY"
            ],
            "Context": "[NVIDIA TF] Enable BF16 L2Loss. GPU support for bfloat16 L2Loss op."
        },
        {
            "Timestamp": "2022-11-18T00:23:19Z",
            "Entity Ids": [
                12981474,
                144114,
                6878204,
                48215717,
                144114
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "0JO6G690EJJY",
                "L7B1KV66D3TI"
            ],
            "Context": "[NVIDIA TF] Enable BF16 L2Loss. GPU support for bfloat16 L2Loss op."
        },
        {
            "Timestamp": "2022-11-18T09:05:26Z",
            "Entity Ids": [
                56741989
            ],
            "Symbol": "Pull Request Merged",
            "Relational IDs": [
                "0JO6G690EJJY",
                "L7B1KV66D3TI",
                "S991XUCIVXEO"
            ],
            "Context": "[NVIDIA TF] Enable BF16 L2Loss. GPU support for bfloat16 L2Loss op."
        },
        {
            "Timestamp": "2022-11-17T16:31:00Z",
            "EntityIds": [
                38138022,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "KU5Q7YR1DKSE"
            ],
            "Context": "Tflite GPU Delegate error for FULLY_CONNECTED. ### 1. System information\r\n\r\n- OS Platform and Distribution: Ubuntu 20.4\r\n- TensorFlow installation: pip package\r\n- TensorFlow library: 2.10\r\n\r\n\r\n### 3. Failure after conversion\r\nIf the conversion is successful, but the generated model is wrong, then state what is wrong:\r\n\r\nModel is successfully converted. However, during benchmark with GPU delegate (1. using latest benchmark binary: https://www.tensorflow.org/lite/performance/measurement#native_benchmark_binary 2. using older benchmark binary), following error appears: \r\n1. ERROR: TfLiteGpuDelegate Init: Unrecognized Write selector\r\n2. ERROR: TfLiteGpuDelegate Init: FULLY_CONNECTED: Amount of input data should match weights width\r\n\r\nWhat is more interesting, if i convert the model using tensorflow 2.3.4, everything works as expected - i can run the benchmark without any problems. \r\nIt seems that starting from tf 2.4, some bug was introduced... \r\n\r\n"
        },
        {
            "Timestamp": "2022-11-17T10:17:39Z",
            "EntityIds": [
                61427290,
                116063290
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "3IND81KVYMCD"
            ],
            "Context": "TensorFlow Dataset.from_generator leaks memory for randomly generated samples (correlated with 'cuda_malloc_async'). <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nBug\n\n### Source\n\nbinary\n\n### Tensorflow Version\n\n2.10, 2.9 tested as well\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\nUbuntu Linux 22.04 and Google Colab tested\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n3.9\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n11.8\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nWhen dataset is generated via `Dataset.from_generator`, samples that have already been used are not removed from GPU memory (or at least this is what I presume), leading to an increased use of VRAM and finally an OOM error as the training proceeds.\r\n\r\nThis is an important bug, since very often (as in this case) someone might use `from_generator` to perform augmentation etc, and It can be easily seen by examining the `GPU_mem_usage`, that the memory usage indeed grows. For some reason, Colab allocates memory straight away to ~8 GB, so the growth is only noticeable after around 300-500 epochs. The problem is way worse when training on real, larger datasets.\r\n\r\nFor a simple summary, just view the graphs on the end of the notebook.\r\n\r\n\r\nNotice (in the notebook), that even though total memory taken on the GPU grows, TensorFlow \"thinks\" it is consuming basically constant amount of memory, which points into the direction of a memory leak.\r\n\r\n\r\nAlso, it takes around 900 epochs to fill almost the whole memory (16 GB in case of Colab). Epochs consist of 1024 images of shape (64, 64, 3), float32 dtype, giving 1024*3*64*64*900*4 bytes allocated in total, which is around 45 GB, so I presume not all memory is leaked, OR leaking data is not the cause of the problem.\r\n\r\nNotice, however, that MobileNetV3 (which I have used in this example) is roughly 18MB in size, and 18MB*900 epochs is basically the aforementioned 16 GB. This could possibly mean that the model's state is leaked somehow, but I have yet to test this hypothesis (for example by using a larger model and checking when the leak happens, i.e. if `num_epochs_until_crash*model_size==gpu_vram_capacity`).\r\n\r\n\r\nAlso, I have tested multiple scenarios locally, and this seems NOT to happen without the `os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'` set, however I have no relevant logs to prove it, since when the flag is absent, whole GPU memory is allocated, and I have no means of monitoring it relevantly, since it doesn't change. I have, however, performed 4 tests and none resulted in OOM error after 1500 epochs, while locally they would usually throw OOM after 400 epochs (RTX 3060Ti).\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nI have provided a minimal example on google colab, can be viewed here:\r\nhttps://colab.research.google.com/drive/1-ANVp8KF9irKvqdR390QNlop-pPhGMrU?usp=sharing\r\nI think the example is self-explanatory.\n```\n\n\n### Relevant log output\n\n_No response_</details>"
        },
        {
            "Timestamp": "2022-11-17T08:54:58Z",
            "EntityIds": [
                86464649,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "L68TGD4MV5HS"
            ],
            "Context": "Fixed some typos in embedding_ops.py. Fixed some typos in embedding_ops.py."
        },
        {
            "Timestamp": "2022-11-17T08:54:58Z",
            "Entity Ids": [
                86464649,
                48215717,
                144114
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "L68TGD4MV5HS",
                "X76DZLP2TION"
            ],
            "Context": "Fixed some typos in embedding_ops.py. Fixed some typos in embedding_ops.py."
        },
        {
            "Timestamp": "2022-11-16T18:19:09Z",
            "EntityIds": [
                20158647,
                86464649
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "O7NWEJ58R4RL"
            ],
            "Context": "TF2.11 Release Estimate?. <details><summary>Click to expand!</summary> \n \n ### Issue Type\n\nOthers\n\n### Source\n\nsource\n\n### Tensorflow Version\n\nTF2.11\n\n### Custom Code\n\nNo\n\n### OS Platform and Distribution\n\n_No response_\n\n### Mobile device\n\n_No response_\n\n### Python version\n\n_No response_\n\n### Bazel version\n\n_No response_\n\n### GCC/Compiler version\n\n_No response_\n\n### CUDA/cuDNN version\n\n_No response_\n\n### GPU model and memory\n\n_No response_\n\n### Current Behaviour?\n\n```shell\nNoticed that there has been activity on the TF2.11 branch for release, and wanted to inquire if there was an ETA on this particular release, or a timeline on when we should expect another RC for TF2.11?\n```\n\n\n### Standalone code to reproduce the issue\n\n```shell\nN/A\n```\n\n\n### Relevant log output\n\n_No response_</details>"
        },
        {
            "Timestamp": "2022-11-16T18:11:12Z",
            "EntityIds": [
                25738889,
                111861663
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "5D1UNB7Z6RQM"
            ],
            "Context": "TFLite benchmark tool - example to use input_layer_value_files. Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1.  It must be a bug, a feature request, or a significant problem with the\r\n    documentation (for small docs fixes please send a PR instead).\r\n2.  The form below must be filled out.\r\n3.  It shouldn't be a TensorBoard issue. Those go\r\n    [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n\r\n-   **Have I written custom code (as opposed to using a stock example script\r\n    provided in TensorFlow)**: No\r\n-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 20.04\r\n-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue\r\n    happens on a mobile device**:\r\n-   **TensorFlow installed from (source or binary)**: Source\r\n-   **TensorFlow version (use command below)**: \r\n-   **Python version**:\r\n-   **Bazel version (if compiling from source)**:\r\n-   **GCC/Compiler version (if compiling from source)**:\r\n-   **CUDA/cuDNN version**:\r\n-   **GPU model and memory**:\r\n-   **Exact command to reproduce**:\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with:\r\n\r\n```bash\r\npython -c \"import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)\"\r\n```\r\n\r\n### Describe the problem\r\nI am trying to use the Android TFLite benchmark tool to run inference time analysis for my TFLite model. Going through the [repo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark), I am interested in passing custom inputs to the benchmark tool. I am specifically looking for how to use input_layer_value_files flag. Could you provide an example of a sample file? Thanks!\r\n\r\n### Source code / logs\r\n-\r\n"
        },
        {
            "Timestamp": "2022-11-16T15:23:07Z",
            "EntityIds": [
                8885699,
                116063290
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "PSC0LNG6ZKRB"
            ],
            "Context": "Tensorflow 2.10 cannot be installed using `poetry` on linux aarch64. <details><summary>Click to expand!</summary> \r\n \r\n ### Issue Type\r\n\r\nBuild/Install\r\n\r\n### Source\r\n\r\nbinary\r\n\r\n### Tensorflow Version\r\n\r\n2.10.0\r\n\r\n### Custom Code\r\n\r\nNo\r\n\r\n### OS Platform and Distribution\r\n\r\nlinux aarch64\r\n\r\n### Mobile device\r\n\r\n_No response_\r\n\r\n### Python version\r\n\r\n3.8\r\n\r\n### Bazel version\r\n\r\n_No response_\r\n\r\n### GCC/Compiler version\r\n\r\n_No response_\r\n\r\n### CUDA/cuDNN version\r\n\r\n_No response_\r\n\r\n### GPU model and memory\r\n\r\n_No response_\r\n\r\n### Current Behaviour?\r\n\r\nPoetry parses the endpoint at `https://pypi.org/pypi/tensorflow/2.10.0/json` to get dependency metadata. The endpoint returns:\r\n```shell\r\n[\r\n  \"absl-py (>=1.0.0)\",\r\n  \"astunparse (>=1.6.0)\",\r\n  \"flatbuffers (>=2.0)\",\r\n  \"gast (<=0.4.0,>=0.2.1)\",\r\n  \"google-pasta (>=0.1.1)\",\r\n  \"h5py (>=2.9.0)\",\r\n  \"keras-preprocessing (>=1.1.1)\",\r\n  \"libclang (>=13.0.0)\",\r\n  \"numpy (>=1.20)\",\r\n  \"opt-einsum (>=2.3.2)\",\r\n  \"packaging\",\r\n  \"protobuf (<3.20,>=3.9.2)\",\r\n  \"setuptools\",\r\n  \"six (>=1.12.0)\",\r\n  \"termcolor (>=1.1.0)\",\r\n  \"typing-extensions (>=3.6.6)\",\r\n  \"wrapt (>=1.11.0)\",\r\n  \"tensorflow-io-gcs-filesystem (>=0.23.1)\",\r\n  \"grpcio (<2.0,>=1.24.3)\",\r\n  \"tensorboard (<2.11,>=2.10)\",\r\n  \"tensorflow-estimator (<2.11,>=2.10.0)\",\r\n  \"keras (<2.11,>=2.10.0)\"\r\n]\r\n```\r\n\r\nHowever, the linux aarch64 wheel has different dependencies, namely `tensorflow-cpu-aws`. Since the dependency metadata is split across wheels, package managers like `poetry` are unable to resolve them correctly. \r\n\r\n**The solution is to have a unified list of dependencies across wheels, with PEP508 environment markers to specify platform-specific dependencies.**\r\n\r\n### Standalone code to reproduce the issue\r\n\r\n```shell\r\npoetry add tensorflow@2.10.0\r\npoetry shell\r\npython -c \"import tensorflow\"\r\n```\r\n\r\n\r\n### Relevant log output\r\n\r\n```shell\r\n(app-py3.8) root@53e9f35d6529:/app# python -c \"import tensorflow\"\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\nModuleNotFoundError: No module named 'tensorflow'\r\n```\r\n</details>"
        },
        {
            "Timestamp": "2022-11-16T14:16:37Z",
            "EntityIds": [
                78156688,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "O52FT2WQ63MF"
            ],
            "Context": "Fix the endianness issue in v1 frozen graphs in python:lite_test on BE machines. This PR is to add big-endian support to TensorFlow v1 frozen graphs.\r\n \r\nThe `byte_swap_tensor` related code in `tensorflow/python/saved_model/utils_impl.py` has been refactored and function `swap_tensor_in_frozen_graph()` was added to deal with the LE/BE conversion in TensorFlow v1 frozen graphs. This function is invoked in frozen graph read/write process so that TensorFlow v1 frozen graphs would be stored with LE format in files and converted to BE format after being loaded into memory on BE machines.\r\n \r\nThis code change also extended the covered code paths for byte-swapping in read/write process of TensorFlow v2 saved models.\r\n \r\nThis PR could fix `//tensorflow/lite/python:lite_test` on s390x (BE machines) and will not cause any regressions on LE/BE platforms.\r\n\r\nFixes issue https://github.com/tensorflow/tensorflow/issues/57215 .\r\n\r\nSigned-off-by: Kun-Lu <kun.lu@ibm.com>"
        },
        {
            "Timestamp": "2022-11-16T14:16:37Z",
            "Entity Ids": [
                78156688,
                48215717,
                5112267
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "O52FT2WQ63MF",
                "DCN708N0CJ2Y"
            ],
            "Context": "Fix the endianness issue in v1 frozen graphs in python:lite_test on BE machines. This PR is to add big-endian support to TensorFlow v1 frozen graphs.\r\n \r\nThe `byte_swap_tensor` related code in `tensorflow/python/saved_model/utils_impl.py` has been refactored and function `swap_tensor_in_frozen_graph()` was added to deal with the LE/BE conversion in TensorFlow v1 frozen graphs. This function is invoked in frozen graph read/write process so that TensorFlow v1 frozen graphs would be stored with LE format in files and converted to BE format after being loaded into memory on BE machines.\r\n \r\nThis code change also extended the covered code paths for byte-swapping in read/write process of TensorFlow v2 saved models.\r\n \r\nThis PR could fix `//tensorflow/lite/python:lite_test` on s390x (BE machines) and will not cause any regressions on LE/BE platforms.\r\n\r\nFixes issue https://github.com/tensorflow/tensorflow/issues/57215 .\r\n\r\nSigned-off-by: Kun-Lu <kun.lu@ibm.com>"
        },
        {
            "Timestamp": "2022-11-16T13:54:48Z",
            "EntityIds": [
                60800749,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "SWPSLK7309KD"
            ],
            "Context": "Move plugin rule to the XLA directory. Moves the dependency of `//tensorflow/compiler/xla/pjrt:pjrt_plugin_device_client` from `//tensorflow/compiler/plugin` to `//tensorflow/compiler/xla/pjrt/plugin`.\r\n\r\nThe old plugin folder is retained to avoid breaking anyone using that rule for unrelated reasons, but it no longer connects to the xla_client.\r\n\r\n@joker-eph @skye @penpornk "
        },
        {
            "Timestamp": "2022-11-16T13:54:48Z",
            "Entity Ids": [
                60800749,
                48215717,
                348959,
                1835471
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "SWPSLK7309KD",
                "9VKGUD5SMH23"
            ],
            "Context": "Move plugin rule to the XLA directory. Moves the dependency of `//tensorflow/compiler/xla/pjrt:pjrt_plugin_device_client` from `//tensorflow/compiler/plugin` to `//tensorflow/compiler/xla/pjrt/plugin`.\r\n\r\nThe old plugin folder is retained to avoid breaking anyone using that rule for unrelated reasons, but it no longer connects to the xla_client.\r\n\r\n@joker-eph @skye @penpornk "
        },
        {
            "Timestamp": "2022-11-16T21:23:13Z",
            "Entity Ids": [
                56741989
            ],
            "Symbol": "Pull Request Merged",
            "Relational IDs": [
                "SWPSLK7309KD",
                "9VKGUD5SMH23",
                "F4WG5GXDMODA"
            ],
            "Context": "Move plugin rule to the XLA directory. Moves the dependency of `//tensorflow/compiler/xla/pjrt:pjrt_plugin_device_client` from `//tensorflow/compiler/plugin` to `//tensorflow/compiler/xla/pjrt/plugin`.\r\n\r\nThe old plugin folder is retained to avoid breaking anyone using that rule for unrelated reasons, but it no longer connects to the xla_client.\r\n\r\n@joker-eph @skye @penpornk "
        },
        {
            "Timestamp": "2022-11-16T09:33:42Z",
            "EntityIds": [
                9656425,
                73069040
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "0CQEHPDDN8SA"
            ],
            "Context": "there is no operator to calculate the matrix's inverse using tflite. tf.raw_ops.MatrixInverse and tf.linalg.svd is not supported in tflite \r\nBatchMatrixInverse is not available in GraphDef version 1205.\r\n\r\nHence\uff0c how to calculate the matrix's inverse using tflite?\r\n\r\nI need some Op to calculate the  matrix's inverse\r\n\r\n\r\nbest wishes\r\n\r\n\r\n\r\n\r\n"
        },
        {
            "Timestamp": "2022-11-16T01:16:16Z",
            "EntityIds": [
                106367904
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "8KFO8XNKGVDL"
            ],
            "Context": "Broken tests is causing release builds failures - TF 2.11.0. # TODO(b/259319686) : Disable the test"
        },
        {
            "Timestamp": "2022-11-16T01:16:16Z",
            "Entity Ids": [
                106367904,
                66660475
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "8KFO8XNKGVDL",
                "IWT92Q89GFXX"
            ],
            "Context": "Broken tests is causing release builds failures - TF 2.11.0. # TODO(b/259319686) : Disable the test"
        },
        {
            "Timestamp": "2022-11-16T01:17:12Z",
            "Entity Ids": [
                323199
            ],
            "Symbol": "Pull Request Merged",
            "Relational IDs": [
                "8KFO8XNKGVDL",
                "IWT92Q89GFXX",
                "C0U9484SLQ0Z"
            ],
            "Context": "Broken tests is causing release builds failures - TF 2.11.0. # TODO(b/259319686) : Disable the test"
        },
        {
            "Timestamp": "2022-11-16T01:02:38Z",
            "EntityIds": [
                3444368,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "S5K2H4WAQJUM"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-16T01:02:38Z",
            "Entity Ids": [
                3444368,
                48215717,
                35820639
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "S5K2H4WAQJUM",
                "W6KL8X7VV1HW"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-15T23:32:04Z",
            "EntityIds": [
                99087793,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "75OI76RTYVTX"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-15T23:32:04Z",
            "Entity Ids": [
                99087793,
                48215717
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "75OI76RTYVTX",
                "25KYZGAK031S"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-15T23:27:47Z",
            "EntityIds": [
                99087793,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "XF0CP34G1Y8N"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-15T23:27:47Z",
            "Entity Ids": [
                99087793,
                48215717
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "XF0CP34G1Y8N",
                "ADMTDJWYBT9L"
            ],
            "Context": "Support writing non conversion report to more filesystems. This PR allows tf trt non conversion report to save in all filesystems supported by tensorflow. This is especially useful when user has a use case to convert a remotely saved model.\r\n\r\nRegarding unit test: I didn't find corresponding unit test for this function in `segment_test.cc`. For this kind of functionality maybe an integration test is more suitable? Please let me know and I could work on that."
        },
        {
            "Timestamp": "2022-11-15T21:19:41Z",
            "EntityIds": [
                73069040,
                48215717
            ],
            "Symbol": "Issue Creation",
            "Relational ID": [
                "6M79FPEELQ73"
            ],
            "Context": "Add c_api.cc under tensorflow/lite/c/. Added c_api.cc file from tensorflow/tensorflow/lite/core/c/c_api.cc under tensorflow/lite/c/ to fix the cmake c build issue.\r\nFixes: #58550"
        },
        {
            "Timestamp": "2022-11-15T21:19:41Z",
            "Entity Ids": [
                73069040,
                48215717,
                2908505
            ],
            "Symbol": "Pull Request Creation",
            "Relational IDs": [
                "6M79FPEELQ73",
                "BE7ZGBEBRCP4"
            ],
            "Context": "Add c_api.cc under tensorflow/lite/c/. Added c_api.cc file from tensorflow/tensorflow/lite/core/c/c_api.cc under tensorflow/lite/c/ to fix the cmake c build issue.\r\nFixes: #58550"
        }
    ]
}